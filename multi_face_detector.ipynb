{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd08182dd0ad4173cdddc5298b982923877be00ee4d2f569af5d6459bf4e939e93c",
   "display_name": "Python 3.7.10 64-bit ('aiffel': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8182dd0ad4173cdddc5298b982923877be00ee4d2f569af5d6459bf4e939e93c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<br>\n",
    "\n",
    "# Multi-Face Detector\n",
    "---\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 데이터셋 준비 <br><br>\n",
    "\n",
    "[WIDER FACE 데이터셋](http://shuoyang1213.me/WIDERFACE/index.html) <br><br>\n",
    "\n",
    "- 32,203개의 이미지, 393,703개의 얼굴 데이터 <br>\n",
    "- train/validataion/test 의 구성비율 40%/10%/50% <br><br>\n",
    "\n",
    "\n",
    "다음 네 개의 파일을 다운로드 <br><br>\n",
    "\n",
    "- (이미지) WIDER Face Training Images <br>\n",
    "- (이미지) WIDER Face Validation Images <br>\n",
    "- (이미지) WIDER Face Testing Images <br>\n",
    "- (레이블) Face annotations (wider_face_slpit) <br><br>\n",
    "\n",
    "\n",
    "여기서 레이블은 bounding box 정보 ! <br>\n",
    "(10개 숫자로 이루어진 데이터) <br><br>\n",
    "\n",
    "x0, y0, w, h, blur, expression, illumination, invalid, occlusion, pose\n",
    "\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 데이터셋 전처리1 : bbox 변환 <br><br>\n",
    "\n",
    "bbox 를 나타내는 레이블 데이터 파일을 분석하여 파싱하고, <br>\n",
    "bbox 데이터 형식을 필요에 맞게 변환하는 코드 작성\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(data):\n",
    "    x0 = int(data[0])\n",
    "    y0 = int(data[1])\n",
    "    w = int(data[2])\n",
    "    h = int(data[3])\n",
    "    return x0, y0, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지별 bbox 정보를 wieder_face_train_bbox.txt 파일로 파싱 후,\n",
    "# 이미지별 bbox 정보 리스트를 추출하는 함수 작성\n",
    "\n",
    "def parse_widerface(config_path):\n",
    "    boxes_per_img = []\n",
    "    with open(config_path) as fp:\n",
    "        line = fp.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            num_of_obj = int(fp.readline())\n",
    "            boxes = []\n",
    "            for i in range(num_of_obj):\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                if w == 0:\n",
    "                    # remove boxes with no width\n",
    "                    continue\n",
    "                if h == 0:\n",
    "                    # remove boxes with no height\n",
    "                    continue\n",
    "                # Because our network is outputting 7x7 grid then it's not worth processing images with more than\n",
    "                # 5 faces because it's highly probable they are close to each other.\n",
    "                # You could remove this filter if you decide to switch to larger grid (like 14x14)\n",
    "                # Don't worry about number of train data because even with this filter we have around 16k samples\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            if num_of_obj == 0:\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            boxes_per_img.append((line.strip(), boxes))\n",
    "            line = fp.readline()\n",
    "            cnt += 1\n",
    "\n",
    "    return boxes_per_img"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "#### bbox 표현형식 변환 <br><br>\n",
    "\n",
    "x, y, w, h -> x_min, y_min, x_max, y_max\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일 디코드하는 함수 작성\n",
    "\n",
    "def process_image(image_file):\n",
    "    image_string = tf.io.read_file(image_file)\n",
    "    try:\n",
    "        image_data = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        return 0, image_string, image_data\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        logging.info('{}: Invalid JPEG data or crop window'.format(image_file))\n",
    "        return 1, image_string, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, w, h 형식에서 x_min, y_min, x_max, y_max 형식으로 변환하는 함수 작성\n",
    "\n",
    "def xywh_to_voc(file_name, boxes, image_data):\n",
    "    shape = image_data.shape\n",
    "    image_info = {}\n",
    "    image_info['filename'] = file_name\n",
    "    image_info['width'] = shape[1]\n",
    "    image_info['height'] = shape[0]\n",
    "    image_info['depth'] = 3\n",
    "\n",
    "    difficult = []\n",
    "    classes = []\n",
    "    xmin, ymin, xmax, ymax = [], [], [], []\n",
    "\n",
    "    for box in boxes:\n",
    "        classes.append(1)\n",
    "        difficult.append(0)\n",
    "        xmin.append(box[0])\n",
    "        ymin.append(box[1])\n",
    "        xmax.append(box[0] + box[2])\n",
    "        ymax.append(box[1] + box[3])\n",
    "    image_info['class'] = classes\n",
    "    image_info['xmin'] = xmin\n",
    "    image_info['ymin'] = ymin\n",
    "    image_info['xmax'] = xmax\n",
    "    image_info['ymax'] = ymax\n",
    "    image_info['difficult'] = difficult\n",
    "\n",
    "    return image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------\n",
      "{'filename': '/home/ssac29/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_849.jpg', 'width': 1024, 'height': 1385, 'depth': 3, 'class': [1], 'xmin': [449], 'ymin': [330], 'xmax': [571], 'ymax': [479], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/ssac29/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_Parade_0_904.jpg', 'width': 1024, 'height': 1432, 'depth': 3, 'class': [1], 'xmin': [361], 'ymin': [98], 'xmax': [624], 'ymax': [437], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/ssac29/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_799.jpg', 'width': 1024, 'height': 768, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [78, 78, 113, 134, 163, 201, 182, 245, 304, 328, 389, 406, 436, 522, 643, 653, 793, 535, 29, 3, 20], 'ymin': [221, 238, 212, 260, 250, 218, 266, 279, 265, 295, 281, 293, 290, 328, 320, 224, 337, 311, 220, 232, 215], 'xmax': [85, 92, 124, 149, 177, 211, 197, 263, 320, 344, 406, 427, 458, 543, 666, 670, 816, 551, 40, 14, 32], 'ymax': [229, 255, 227, 275, 267, 230, 283, 294, 282, 315, 300, 314, 307, 346, 342, 249, 367, 328, 235, 247, 231], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/ssac29/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_117.jpg', 'width': 1024, 'height': 682, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [69, 227, 296, 353, 885, 819, 727, 598, 740], 'ymin': [359, 382, 305, 280, 377, 391, 342, 246, 308], 'xmax': [119, 283, 340, 393, 948, 853, 764, 631, 785], 'ymax': [395, 425, 331, 316, 418, 434, 373, 275, 341], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/ssac29/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_778.jpg', 'width': 1024, 'height': 852, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [27, 63, 64, 88, 231, 263, 367, 198, 293, 412, 441, 475, 510, 576, 577, 595, 570, 645, 719, 791, 884, 898, 945, 922, 743, 841, 980, 1001, 488, 586, 669, 744, 803, 294, 203], 'ymin': [226, 95, 63, 13, 1, 122, 68, 98, 161, 36, 23, 40, 23, 30, 71, 94, 126, 171, 98, 154, 97, 48, 89, 38, 71, 18, 56, 107, 2, 1, 1, 2, 3, 2, 0], 'xmax': [60, 79, 81, 104, 244, 277, 382, 213, 345, 426, 458, 489, 524, 592, 593, 611, 583, 697, 730, 845, 900, 913, 960, 937, 754, 857, 993, 1015, 500, 601, 681, 762, 821, 305, 216], 'ymax': [262, 114, 81, 28, 14, 142, 91, 116, 220, 56, 36, 61, 40, 45, 92, 114, 142, 229, 113, 203, 118, 69, 109, 54, 89, 34, 76, 120, 20, 18, 16, 17, 20, 12, 14], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 이미지별 수정된 bbox 데이터 확인\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "dataset_path = os.getenv('HOME')+'/aiffel/face_detector/widerface'\n",
    "anno_txt = 'wider_face_train_bbx_gt.txt'\n",
    "file_path = 'WIDER_train'\n",
    "for i, info in enumerate(parse_widerface(os.path.join(dataset_path, 'wider_face_split', anno_txt))):\n",
    "    print('--------------------')\n",
    "    image_file = os.path.join(dataset_path, file_path, 'images', info[0])\n",
    "    error, image_string, image_data = process_image(image_file)\n",
    "    boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "    print(boxes)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 데이터셋 전처리2 : 텐서플로우 데이터셋 만들기 <br><br>\n",
    "\n",
    "대용량 데이터셋 처리속도 향상을 위해, <br>\n",
    "__tfrecord__ 데이터셋으로 변환 ! <br><br>\n",
    "\n",
    "\n",
    "1 개 데이터의 단위 인스턴스를 생성하는 메소드 : ```tf.train.Example()``` <br><br>\n",
    "\n",
    "- 'filename' <br>\n",
    "- 'height' <br>\n",
    "- 'width' <br>\n",
    "- 'classes' <br>\n",
    "- 'x_mins' <br>\n",
    "- 'y_mins' <br>\n",
    "- 'x_maxes' <br>\n",
    "- 'y_maxes' <br>\n",
    "- 'image_raw'\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 개 데이터의 단위를 생성하는 메소드 작성\n",
    "\n",
    "def make_example(image_string, image_info_list):\n",
    "\n",
    "    for info in image_info_list:\n",
    "        filename = info['filename']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        depth = info['depth']\n",
    "        classes = info['class']\n",
    "        xmin = info['xmin']\n",
    "        ymin = info['ymin']\n",
    "        xmax = info['xmax']\n",
    "        ymax = info['ymax']\n",
    "\n",
    "    if isinstance(image_string, type(tf.constant(0))):\n",
    "        encoded_image = [image_string.numpy()]\n",
    "    else:\n",
    "        encoded_image = [image_string]\n",
    "\n",
    "    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename':tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n",
    "        'height':tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'width':tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'classes':tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "        'x_mins':tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
    "        'y_mins':tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
    "        'x_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
    "        'y_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
    "        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 12880/12880 [00:49<00:00, 260.22it/s]\n",
      "100%|██████████| 3226/3226 [00:12<00:00, 267.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# make_example 메소드로 만든 1 개의 데이터셋 example 을 <br>\n",
    "# serialize 하여 바이너리파일로 생성\n",
    "\n",
    "import logging\n",
    "import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "rootPath = os.getenv('HOME')+'/aiffel/face_detector'\n",
    "dataset_path = 'widerface'\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    logging.info('Please define valid dataset path.')\n",
    "else:\n",
    "    logging.info('Loading {}'.format(dataset_path))\n",
    "\n",
    "logging.info('Reading configuration...')\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    output_file = rootPath + '/dataset/train_mask.tfrecord' if split == 'train' else rootPath + '/dataset/val_mask.tfrecord'\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "\n",
    "        counter = 0\n",
    "        skipped = 0\n",
    "        anno_txt = 'wider_face_train_bbx_gt.txt' if split == 'train' else 'wider_face_val_bbx_gt.txt'\n",
    "        file_path = 'WIDER_train' if split == 'train' else 'WIDER_val'\n",
    "        for info in tqdm.tqdm(parse_widerface(os.path.join(rootPath, dataset_path, 'wider_face_split', anno_txt))):\n",
    "            image_file = os.path.join(rootPath, dataset_path, file_path, 'images', info[0])\n",
    "\n",
    "            error, image_string, image_data = process_image(image_file)\n",
    "            boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "\n",
    "            if not error:\n",
    "                tf_example = make_example(image_string, [boxes])\n",
    "\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "                counter += 1\n",
    "\n",
    "            else:\n",
    "                skipped += 1\n",
    "                logging.info('Skipped {:d} of {:d} images.'.format(skipped, counter))\n",
    "\n",
    "    logging.info('Wrote {} images to {}'.format(counter, output_file))"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "#### 전처리 과정 파이썬 모듈화 <br>\n",
    "\n",
    "전처리1, 전처리2 과정을 tf_dataset_preprocess.py 파일에 모듈화 <br><br>\n",
    "\n",
    "```cd ~/aiffel/face_detector && python tf_dataset_preprocess.py```\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## Single Shot multi-box Detector (SSD) 모델 <br><br>\n",
    "\n",
    "\n",
    "### SSD 모델의 특징 : prior box (anchor box) <br><br>\n",
    "\n",
    "SSD 모델은 prior box (anchor box) 를 필요로 함 <br><br>\n",
    "\n",
    "\n",
    "### prior box (anchor box) <br><br>\n",
    "\n",
    "- object 가 존재할 만한 다양한 크기의 box 좌표 및 클래스 정보를 <br>\n",
    "    일정 갯수 미리 고정해 두는 것 <br><br>\n",
    "\n",
    "- ground truth 에 해당하는 bounding box 와의 IOU 계산 결과가 <br>\n",
    "    일정값(0.5) 이상이 되도록 겹치는 prior box 를 선택하는 방식 <br><br>\n",
    "\n",
    "    RCNN 계열의 sliding window 방식보다 속도가 빠르고, <br>\n",
    "    RCNN 계열의 sliding window 방식과 유사한 정도의 정확도 <br><br>\n",
    "\n",
    "- 각 클래스별로 기준이 되는 feature map 을 생성하고, <br>\n",
    "    feature map 별로 순회 하며 prior box 생성 <br><br>\n",
    "\n",
    "\n",
    "참고. <br>\n",
    "[Understand Single Shot MultiBox Detector (SSD) and Implement It in Pytorch](https://medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad) <br>\n",
    "[Understanding SSD MultiBox Real-Time Obeject Detection in Deep Learning](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab)\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 모델 구현 : priors box 구현\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'input_size': (240, 320),\n",
       " 'dataset_path': 'dataset/train_mask.tfrecord',\n",
       " 'val_path': 'dataset/val_mask.tfrecord',\n",
       " 'dataset_len': 12880,\n",
       " 'val_len': 3226,\n",
       " 'using_crop': True,\n",
       " 'using_bin': True,\n",
       " 'using_flip': True,\n",
       " 'using_distort': True,\n",
       " 'using_normalizing': True,\n",
       " 'labels_list': ['background', 'face'],\n",
       " 'min_sizes': [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
       " 'steps': [8, 16, 32, 64],\n",
       " 'match_thresh': 0.45,\n",
       " 'variances': [0.1, 0.2],\n",
       " 'clip': False,\n",
       " 'base_channel': 16,\n",
       " 'resume': False,\n",
       " 'epoch': 100,\n",
       " 'init_lr': 0.01,\n",
       " 'lr_decay_epoch': [50, 70],\n",
       " 'lr_rate': 0.1,\n",
       " 'warmup_epoch': 5,\n",
       " 'min_lr': 0.0001,\n",
       " 'weights_decay': 0.0005,\n",
       " 'momentum': 0.9,\n",
       " 'save_freq': 10,\n",
       " 'score_threshold': 0.5,\n",
       " 'nms_threshold': 0.4,\n",
       " 'max_number_keep': 200}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 이번 프로젝트에서 활용할 config 정보를 모아 dict 구조로 정리\n",
    "\n",
    "cfg = {\n",
    "    # general setting\n",
    "    \"batch_size\": 32,\n",
    "    \"input_size\": (240, 320),  # (h,w)\n",
    "\n",
    "    # training dataset\n",
    "    \"dataset_path\": 'dataset/train_mask.tfrecord',  # 'dataset/trainval_mask.tfrecord'\n",
    "    \"val_path\": 'dataset/val_mask.tfrecord',  #\n",
    "    \"dataset_len\": 12880,  # train 6115 , trainval 7954, number of training samples\n",
    "    \"val_len\": 3226,\n",
    "    \"using_crop\": True,\n",
    "    \"using_bin\": True,\n",
    "    \"using_flip\": True,\n",
    "    \"using_distort\": True,\n",
    "    \"using_normalizing\": True,\n",
    "    \"labels_list\": ['background', 'face'],  # xml annotation\n",
    "\n",
    "    # anchor setting\n",
    "    \"min_sizes\":[[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
    "    \"steps\": [8, 16, 32, 64],\n",
    "    \"match_thresh\": 0.45,\n",
    "    \"variances\": [0.1, 0.2],\n",
    "    \"clip\": False,\n",
    "\n",
    "    # network\n",
    "    \"base_channel\": 16,\n",
    "\n",
    "    # training setting\n",
    "    \"resume\": False,  # if False,training from scratch\n",
    "    \"epoch\": 100,\n",
    "    \"init_lr\": 1e-2,\n",
    "    \"lr_decay_epoch\": [50, 70],\n",
    "    \"lr_rate\": 0.1,\n",
    "    \"warmup_epoch\": 5,\n",
    "    \"min_lr\": 1e-4,\n",
    "\n",
    "    \"weights_decay\": 5e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"save_freq\": 10, #frequency of save model weights\n",
    "\n",
    "    # inference\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"nms_threshold\": 0.4,\n",
    "    \"max_number_keep\": 200\n",
    "}\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(240, 320)\n"
     ]
    }
   ],
   "source": [
    "# config 중 prior box 생성 관련된 정보 확인\n",
    "\n",
    "image_sizes = cfg['input_size']\n",
    "min_sizes = cfg[\"min_sizes\"]\n",
    "steps = cfg[\"steps\"]\n",
    "clip = cfg[\"clip\"]\n",
    "\n",
    "if isinstance(image_sizes, int):\n",
    "    image_sizes = (image_sizes, image_sizes)\n",
    "elif isinstance(image_sizes, tuple):\n",
    "    image_sizes = image_sizes\n",
    "else:\n",
    "    raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "print(image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[30, 40], [15, 20], [8, 10], [4, 5]]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 각 클래스 별로 기준이 되는 feature map 생성\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "for m in range(4):\n",
    "    if (steps[m] != pow(2, (m + 3))):\n",
    "        print(\"steps must be [8,16,32,64]\")\n",
    "        sys.exit()\n",
    "\n",
    "assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "\n",
    "feature_maps = [\n",
    "    [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "    for step in steps]\n",
    "\n",
    "feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "17680"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# feature map 별로 순회 하며 prior box 생성\n",
    "\n",
    "anchors = []\n",
    "num_box_fm_cell=[]\n",
    "for k, f in enumerate(feature_maps):\n",
    "    num_box_fm_cell.append(len(min_sizes[k]))\n",
    "    for i, j in product(range(f[0]), range(f[1])):\n",
    "        for min_size in min_sizes[k]:\n",
    "            if isinstance(min_size, int):\n",
    "                min_size = (min_size, min_size)\n",
    "            elif isinstance(min_size, tuple):\n",
    "                min_size=min_size\n",
    "            else:\n",
    "                raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "            s_kx = min_size[1] / image_sizes[1]\n",
    "            s_ky = min_size[0] / image_sizes[0]\n",
    "            cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "            cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "            anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4420, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# 생성된 prior box 를 numpy 를 통해 reshape\n",
    "\n",
    "import numpy as np\n",
    "priors = np.asarray(anchors).reshape([-1, 4])\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.0125    , 0.01666667, 0.03125   , 0.04166667],\n",
       "       [0.0125    , 0.01666667, 0.05      , 0.06666667],\n",
       "       [0.0125    , 0.01666667, 0.075     , 0.1       ],\n",
       "       ...,\n",
       "       [0.9       , 0.93333333, 0.4       , 0.53333333],\n",
       "       [0.9       , 0.93333333, 0.6       , 0.8       ],\n",
       "       [0.9       , 0.93333333, 0.8       , 1.06666667]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# 생성된 prior box 확인\n",
    "\n",
    "priors"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "#### prior box 생성 과정 파이썬 모듈화 <br>\n",
    "\n",
    "모델 구현 중 prior box 를 생성하는 과정을 make_prior_box.py 파일에 모듈화 <br><br>\n",
    "\n",
    "```cd ~/aiffel/face_detector && python make_prior_box.py```\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior_box() 메소드 생성\n",
    "\n",
    "def prior_box(cfg,image_sizes=None):\n",
    "    \"\"\"prior box\"\"\"\n",
    "    if image_sizes is None:\n",
    "        image_sizes = cfg['input_size']\n",
    "    min_sizes=cfg[\"min_sizes\"]\n",
    "    steps=cfg[\"steps\"]\n",
    "    clip=cfg[\"clip\"]\n",
    "\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    for m in range(4):\n",
    "        if (steps[m] != pow(2, (m + 3))):\n",
    "            print(\"steps must be [8,16,32,64]\")\n",
    "            sys.exit()\n",
    "\n",
    "    assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "    feature_maps = [\n",
    "        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "        for step in steps]\n",
    "\n",
    "    anchors = []\n",
    "    num_box_fm_cell=[]\n",
    "    for k, f in enumerate(feature_maps):\n",
    "        num_box_fm_cell.append(len(min_sizes[k]))\n",
    "        for i, j in product(range(f[0]), range(f[1])):\n",
    "            for min_size in min_sizes[k]:\n",
    "                if isinstance(min_size, int):\n",
    "                    min_size = (min_size, min_size)\n",
    "                elif isinstance(min_size, tuple):\n",
    "                    min_size=min_size\n",
    "                else:\n",
    "                    raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "                s_kx = min_size[1] / image_sizes[1]\n",
    "                s_ky = min_size[0] / image_sizes[0]\n",
    "                cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "                cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "                anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "    output = np.asarray(anchors).reshape([-1, 4])\n",
    "\n",
    "    if clip:\n",
    "        output = np.clip(output, 0, 1)\n",
    "    return output,num_box_fm_cell"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 모델 구현 : SSD 구현 <br><br>\n",
    "\n",
    "위에서 생성한 prior box 생성 메소드를 이용하여, <br>\n",
    "SSD (Single Shot MultiBox Detector) 모델을 구현\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD 모델 내부에서 사용하는 레이어들을 블록으로 생성\n",
    "# _conv_block\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1), use_bn=True, padding=None, block_id=None):\n",
    "    \"\"\"Adds an initial convolution layer (with batch normalization and relu).\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (2, 2):\n",
    "        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='valid',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='same',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(inputs)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD 모델 내부에서 사용하는 레이어들을 블록으로 생성\n",
    "# _depthwise_conv_block\n",
    "\n",
    "def _depthwise_conv_block(inputs, pointwise_conv_filters,\n",
    "                          depth_multiplier=1, strides=(1, 1), use_bn=True, block_id=None):\n",
    "    \"\"\"Adds a depthwise convolution block.\n",
    "        # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (1, 1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "\n",
    "    x = tf.keras.layers.DepthwiseConv2D((3, 3),\n",
    "                                        padding='same' if strides == (1, 1) else 'valid',\n",
    "                                        depth_multiplier=depth_multiplier,\n",
    "                                        strides=strides,\n",
    "                                        use_bias=False if use_bn else True,\n",
    "                                        name='conv_dw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = tf.keras.layers.ReLU(name='conv_dw_%d_relu' % block_id)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(pointwise_conv_filters, (1, 1),\n",
    "                               padding='same',\n",
    "                               use_bias=False if use_bn else True,\n",
    "                               strides=(1, 1),\n",
    "                               name='conv_pw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_pw_%d_relu' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD 모델 내부에서 사용하는 레이어들을 블록으로 생성\n",
    "# _branch_block\n",
    "\n",
    "def _branch_block(input, filters):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv2D(filters * 2, kernel_size=(3, 3), padding='same')(input)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)([x, x1])\n",
    "\n",
    "    return tf.keras.layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD 모델 내부에서 사용하는 레이어들을 블록으로 생성\n",
    "# _create_hear_block\n",
    "\n",
    "def _create_head_block(inputs, filters, strides=(1, 1), block_id=None):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD 모델 내부에서 사용하는 레이어들을 블록으로 생성\n",
    "# _compute_heads\n",
    "\n",
    "def _compute_heads(x, idx, num_class, num_cell):\n",
    "    \"\"\" Compute outputs of classification and regression heads\n",
    "    Args:\n",
    "        x: the input feature map\n",
    "        idx: index of the head layer\n",
    "    Returns:\n",
    "        conf: output of the idx-th classification head\n",
    "        loc: output of the idx-th regression head\n",
    "    \"\"\"\n",
    "    conf = _create_head_block(inputs=x, filters=num_cell[idx] * num_class)\n",
    "    conf = tf.keras.layers.Reshape((-1, num_class))(conf)\n",
    "    loc = _create_head_block(inputs=x, filters=num_cell[idx] * 4)\n",
    "    loc = tf.keras.layers.Reshape((-1, 4))(loc)\n",
    "\n",
    "    return conf, loc"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "레이어 블록 준비 끝. <br>\n",
    "레이어 블록을 활용하여 SSD 모델 생성.\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어 블록을 활용하여 SSD 모델 구현\n",
    "\n",
    "def SsdModel(cfg, num_cell, training=False, name='ssd_model'):\n",
    "    image_sizes = cfg['input_size']   if training else None\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    elif image_sizes == None:\n",
    "        image_sizes = (None, None)\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    base_channel = cfg[\"base_channel\"]\n",
    "    num_class = len(cfg['labels_list'])\n",
    "\n",
    "    x = inputs = tf.keras.layers.Input(shape=[image_sizes[0], image_sizes[1], 3], name='input_image')\n",
    "\n",
    "    x = _conv_block(x, base_channel, strides=(2, 2))  # 120*160*16\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(2, 2))  # 60*80\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(2, 2))  # 30*40\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x1 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _conv_block(x, base_channel * 8, strides=(2, 2))  # 15*20\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x2 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 8*10\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(1, 1))\n",
    "    x3 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 4*5\n",
    "    x4 = _branch_block(x, base_channel)\n",
    "\n",
    "    extra_layers = [x1, x2, x3, x4]\n",
    "\n",
    "    confs = []\n",
    "    locs = []\n",
    "\n",
    "    head_idx = 0\n",
    "    assert len(extra_layers) == len(num_cell)\n",
    "    for layer in extra_layers:\n",
    "        conf, loc = _compute_heads(layer, head_idx, num_class, num_cell)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "\n",
    "        head_idx += 1\n",
    "\n",
    "    confs = tf.keras.layers.Concatenate(axis=1, name=\"face_classes\")(confs)\n",
    "    locs = tf.keras.layers.Concatenate(axis=1, name=\"face_boxes\")(locs)\n",
    "\n",
    "    predictions = tf.keras.layers.Concatenate(axis=2, name='predictions')([locs, confs])\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                    \n__________________________________________________________________________________________________\nconv_relu_16 (ReLU)             (None, None, None, 3 0           conv_bn_16[0][0]                 \n__________________________________________________________________________________________________\nconv_pad_17 (ZeroPadding2D)     (None, None, None, 3 0           conv_relu_16[0][0]               \n__________________________________________________________________________________________________\nconv_17 (Conv2D)                (None, None, None, 3 9216        conv_pad_17[0][0]                \n__________________________________________________________________________________________________\nconv_bn_17 (BatchNormalization) (None, None, None, 3 128         conv_17[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_17 (ReLU)             (None, None, None, 3 0           conv_bn_17[0][0]                 \n__________________________________________________________________________________________________\nconv_18 (Conv2D)                (None, None, None, 3 9216        conv_relu_17[0][0]               \n__________________________________________________________________________________________________\nconv_bn_18 (BatchNormalization) (None, None, None, 3 128         conv_18[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_18 (ReLU)             (None, None, None, 3 0           conv_bn_18[0][0]                 \n__________________________________________________________________________________________________\nconv_pad_19 (ZeroPadding2D)     (None, None, None, 3 0           conv_relu_18[0][0]               \n__________________________________________________________________________________________________\nconv_19 (Conv2D)                (None, None, None, 6 18432       conv_pad_19[0][0]                \n__________________________________________________________________________________________________\nconv_bn_19 (BatchNormalization) (None, None, None, 6 256         conv_19[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_19 (ReLU)             (None, None, None, 6 0           conv_bn_19[0][0]                 \n__________________________________________________________________________________________________\nconv_20 (Conv2D)                (None, None, None, 6 36864       conv_relu_19[0][0]               \n__________________________________________________________________________________________________\nconv_bn_20 (BatchNormalization) (None, None, None, 6 256         conv_20[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_20 (ReLU)             (None, None, None, 6 0           conv_bn_20[0][0]                 \n__________________________________________________________________________________________________\nconv_21 (Conv2D)                (None, None, None, 6 36864       conv_relu_20[0][0]               \n__________________________________________________________________________________________________\nconv_bn_21 (BatchNormalization) (None, None, None, 6 256         conv_21[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_21 (ReLU)             (None, None, None, 6 0           conv_bn_21[0][0]                 \n__________________________________________________________________________________________________\nconv_22 (Conv2D)                (None, None, None, 6 36864       conv_relu_21[0][0]               \n__________________________________________________________________________________________________\nconv_bn_22 (BatchNormalization) (None, None, None, 6 256         conv_22[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_22 (ReLU)             (None, None, None, 6 0           conv_bn_22[0][0]                 \n__________________________________________________________________________________________________\nconv_pad_23 (ZeroPadding2D)     (None, None, None, 6 0           conv_relu_22[0][0]               \n__________________________________________________________________________________________________\nconv_23 (Conv2D)                (None, None, None, 1 73728       conv_pad_23[0][0]                \n__________________________________________________________________________________________________\nconv_bn_23 (BatchNormalization) (None, None, None, 1 512         conv_23[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_23 (ReLU)             (None, None, None, 1 0           conv_bn_23[0][0]                 \n__________________________________________________________________________________________________\nconv_24 (Conv2D)                (None, None, None, 1 147456      conv_relu_23[0][0]               \n__________________________________________________________________________________________________\nconv_bn_24 (BatchNormalization) (None, None, None, 1 512         conv_24[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_24 (ReLU)             (None, None, None, 1 0           conv_bn_24[0][0]                 \n__________________________________________________________________________________________________\nconv_25 (Conv2D)                (None, None, None, 1 147456      conv_relu_24[0][0]               \n__________________________________________________________________________________________________\nconv_bn_25 (BatchNormalization) (None, None, None, 1 512         conv_25[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_25 (ReLU)             (None, None, None, 1 0           conv_bn_25[0][0]                 \n__________________________________________________________________________________________________\nconv_pad_26 (ZeroPadding2D)     (None, None, None, 1 0           conv_relu_25[0][0]               \n__________________________________________________________________________________________________\nconv_dw_26 (DepthwiseConv2D)    (None, None, None, 1 1152        conv_pad_26[0][0]                \n__________________________________________________________________________________________________\nconv_dw_26_bn (BatchNormalizati (None, None, None, 1 512         conv_dw_26[0][0]                 \n__________________________________________________________________________________________________\nconv_dw_26_relu (ReLU)          (None, None, None, 1 0           conv_dw_26_bn[0][0]              \n__________________________________________________________________________________________________\nconv_pw_26 (Conv2D)             (None, None, None, 2 32768       conv_dw_26_relu[0][0]            \n__________________________________________________________________________________________________\nconv_pw_26_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_26[0][0]                 \n__________________________________________________________________________________________________\nconv_pw_26_relu (ReLU)          (None, None, None, 2 0           conv_pw_26_bn[0][0]              \n__________________________________________________________________________________________________\nconv_dw_27 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pw_26_relu[0][0]            \n__________________________________________________________________________________________________\nconv_dw_27_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_27[0][0]                 \n__________________________________________________________________________________________________\nconv_dw_27_relu (ReLU)          (None, None, None, 2 0           conv_dw_27_bn[0][0]              \n__________________________________________________________________________________________________\nconv_pw_27 (Conv2D)             (None, None, None, 2 65536       conv_dw_27_relu[0][0]            \n__________________________________________________________________________________________________\nconv_pw_27_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_27[0][0]                 \n__________________________________________________________________________________________________\nconv_pw_27_relu (ReLU)          (None, None, None, 2 0           conv_pw_27_bn[0][0]              \n__________________________________________________________________________________________________\nconv_pad_28 (ZeroPadding2D)     (None, None, None, 2 0           conv_pw_27_relu[0][0]            \n__________________________________________________________________________________________________\nconv_dw_28 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pad_28[0][0]                \n__________________________________________________________________________________________________\nconv_dw_28_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_28[0][0]                 \n__________________________________________________________________________________________________\nconv_dw_28_relu (ReLU)          (None, None, None, 2 0           conv_dw_28_bn[0][0]              \n__________________________________________________________________________________________________\nconv_pw_28 (Conv2D)             (None, None, None, 2 65536       conv_dw_28_relu[0][0]            \n__________________________________________________________________________________________________\nconv_pw_28_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_28[0][0]                 \n__________________________________________________________________________________________________\nconv_pw_28_relu (ReLU)          (None, None, None, 2 0           conv_pw_28_bn[0][0]              \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, None, None, 1 9232        conv_relu_22[0][0]               \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, None, None, 1 18448       conv_relu_25[0][0]               \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, None, None, 1 36880       conv_pw_27_relu[0][0]            \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, None, None, 1 36880       conv_pw_28_relu[0][0]            \n__________________________________________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)       (None, None, None, 1 0           conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, None, None, 1 0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)       (None, None, None, 1 0           conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)       (None, None, None, 1 0           conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, None, None, 3 18464       conv_relu_22[0][0]               \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, None, None, 3 36896       conv_relu_25[0][0]               \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_6[0][0]              \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, None, None, 3 73760       conv_pw_27_relu[0][0]            \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_7[0][0]              \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, None, None, 3 73760       conv_pw_28_relu[0][0]            \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, None, None, 4 0           conv2d_13[0][0]                  \n                                                                 conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, None, None, 4 0           conv2d_16[0][0]                  \n                                                                 conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, None, None, 4 0           conv2d_19[0][0]                  \n                                                                 conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, None, None, 4 0           conv2d_22[0][0]                  \n                                                                 conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nre_lu_4 (ReLU)                  (None, None, None, 4 0           concatenate_4[0][0]              \n__________________________________________________________________________________________________\nre_lu_5 (ReLU)                  (None, None, None, 4 0           concatenate_5[0][0]              \n__________________________________________________________________________________________________\nre_lu_6 (ReLU)                  (None, None, None, 4 0           concatenate_6[0][0]              \n__________________________________________________________________________________________________\nre_lu_7 (ReLU)                  (None, None, None, 4 0           concatenate_7[0][0]              \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, None, None, 1 5196        re_lu_4[0][0]                    \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, None, None, 8 3464        re_lu_5[0][0]                    \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, None, None, 8 3464        re_lu_6[0][0]                    \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, None, None, 1 5196        re_lu_7[0][0]                    \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, None, None, 6 2598        re_lu_4[0][0]                    \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, None, None, 4 1732        re_lu_5[0][0]                    \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, None, None, 4 1732        re_lu_6[0][0]                    \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, None, None, 6 2598        re_lu_7[0][0]                    \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, None, 4)      0           conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nreshape_3 (Reshape)             (None, None, 4)      0           conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nreshape_5 (Reshape)             (None, None, 4)      0           conv2d_29[0][0]                  \n__________________________________________________________________________________________________\nreshape_7 (Reshape)             (None, None, 4)      0           conv2d_31[0][0]                  \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, None, 2)      0           conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nreshape_2 (Reshape)             (None, None, 2)      0           conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nreshape_4 (Reshape)             (None, None, 2)      0           conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nreshape_6 (Reshape)             (None, None, 2)      0           conv2d_30[0][0]                  \n__________________________________________________________________________________________________\nface_boxes (Concatenate)        (None, None, 4)      0           reshape_1[0][0]                  \n                                                                 reshape_3[0][0]                  \n                                                                 reshape_5[0][0]                  \n                                                                 reshape_7[0][0]                  \n__________________________________________________________________________________________________\nface_classes (Concatenate)      (None, None, 2)      0           reshape[0][0]                    \n                                                                 reshape_2[0][0]                  \n                                                                 reshape_4[0][0]                  \n                                                                 reshape_6[0][0]                  \n__________________________________________________________________________________________________\npredictions (Concatenate)       (None, None, 6)      0           face_boxes[0][0]                 \n                                                                 face_classes[0][0]               \n==================================================================================================\nTotal params: 1,038,956\nTrainable params: 1,034,636\nNon-trainable params: 4,320\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "\n",
    "import os\n",
    "\n",
    "model = SsdModel(cfg, num_cell=[3, 2, 2, 3], training=False)\n",
    "print(len(model.layers))\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "#### SSD 모델 생성 과정 파이썬 모듈화 <br>\n",
    "\n",
    "모델 구현 중 SSD 모델 구조를 생성하는 과정을 tf_build_ssd_model.py 파일에 모듈화 <br><br>\n",
    "\n",
    "```cd ~/aiffel/face_detector && python tf_build_ssd_model.py```\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 학습용 데이터셋 준비 <br><br>\n",
    "\n",
    "SSD 모델 학습을 위한 데이터셋에는 몇 가지 전처리 과정이 필요합니다. <br><br>\n",
    "\n",
    "1. Augmentation 적용 <br>\n",
    "\n",
    "2. Prior box 적용 <br>\n",
    "\n",
    "3. Train/Validation set 분할 및 배치 결정\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Augmentation <br><br>\n",
    "\n",
    "\n",
    "데이터 준비 과정에서 구성한 tfrecordset 형태의 데이터셋에는 아직 data augmentation 적용이 되지 않았습니다. <br>\n",
    "Object Detection 에서 사용하는 다양한 Augmentation 기법을 데이터셋에 적용하면 성능 향상을 기대할 수 있습니다. <br><br>\n",
    "\n",
    "Augmentation 을 위해 <br>\n",
    "```tf.data.TFRecordDataset.map()``` 에서 호출할 메소드들을 생성 <br><br>\n",
    "\n",
    "- _crop <br>\n",
    "- _pad_to_square <br>\n",
    "- _resize <br>\n",
    "- _flip <br>\n",
    "- _distort\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _crop 메소드 생성\n",
    "\n",
    "def _crop(img, labels, max_loop=250):\n",
    "    shape = tf.shape(img)\n",
    "\n",
    "    def matrix_iof(a, b):\n",
    "        \"\"\"\n",
    "        return iof of a and b, numpy version for data augenmentation\n",
    "        \"\"\"\n",
    "        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n",
    "        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n",
    "            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n",
    "        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n",
    "\n",
    "    def crop_loop_body(i, img, labels):\n",
    "        valid_crop = tf.constant(1, tf.int32)\n",
    "\n",
    "        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n",
    "        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n",
    "        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n",
    "        h = w = tf.cast(scale * short_side, tf.int32)\n",
    "        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n",
    "        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n",
    "        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n",
    "        roi = tf.cast(roi, tf.float32)\n",
    "\n",
    "\n",
    "        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n",
    "        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n",
    "        mask_a = tf.reduce_all(\n",
    "            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n",
    "            axis=1)\n",
    "        labels_t = tf.boolean_mask(labels, mask_a)\n",
    "        valid_crop = tf.cond(tf.reduce_any(mask_a),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n",
    "        h_offset = tf.cast(h_offset, tf.float32)\n",
    "        w_offset = tf.cast(w_offset, tf.float32)\n",
    "        labels_t = tf.stack(\n",
    "            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n",
    "             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n",
    "             labels_t[:, 4]], axis=1)\n",
    "\n",
    "        return tf.cond(valid_crop == 1,\n",
    "                       lambda: (max_loop, img_t, labels_t),\n",
    "                       lambda: (i + 1, img, labels))\n",
    "\n",
    "    _, img, labels = tf.while_loop(\n",
    "        lambda i, img, labels: tf.less(i, max_loop),\n",
    "        crop_loop_body,\n",
    "        [tf.constant(-1), img, labels],\n",
    "        shape_invariants=[tf.TensorShape([]),\n",
    "                          tf.TensorShape([None, None, 3]),\n",
    "                          tf.TensorShape([None, 5])])\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _pad_to_square 메소드 생성\n",
    "\n",
    "def _pad_to_square(img):\n",
    "    height = tf.shape(img)[0]\n",
    "    width = tf.shape(img)[1]\n",
    "\n",
    "    def pad_h():\n",
    "        img_pad_h = tf.ones([width - height, width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_h], axis=0)\n",
    "\n",
    "    def pad_w():\n",
    "        img_pad_w = tf.ones([height, height - width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_w], axis=1)\n",
    "\n",
    "    img = tf.case([(tf.greater(height, width), pad_w),\n",
    "                   (tf.less(height, width), pad_h)], default=lambda: img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _resize 메소드 생성\n",
    "\n",
    "def _resize(img, labels, img_dim):\n",
    "    ''' # resize and boxes coordinate to percent'''\n",
    "    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n",
    "    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n",
    "    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n",
    "                     labels[:, 2] / w_f,  labels[:, 3] / h_f] ,axis=1)\n",
    "    locs = tf.clip_by_value(locs, 0, 1.0)\n",
    "    labels = tf.concat([locs, labels[:, 4][:, tf.newaxis]], axis=1)\n",
    "\n",
    "    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n",
    "    if isinstance(img_dim, int):\n",
    "        img_dim = (img_dim, img_dim)\n",
    "    elif isinstance(img_dim,tuple):\n",
    "        img_dim = img_dim\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    def resize(method):\n",
    "        def _resize():\n",
    "            #　size h,w\n",
    "            return tf.image.resize(img, [img_dim[0], img_dim[1]], method=method, antialias=True)\n",
    "        return _resize\n",
    "\n",
    "    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n",
    "                   (tf.equal(resize_case, 1), resize('area')),\n",
    "                   (tf.equal(resize_case, 2), resize('nearest')),\n",
    "                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n",
    "                  default=resize('bilinear'))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _flip 메소드 생성\n",
    "\n",
    "def _flip(img, labels):\n",
    "    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n",
    "\n",
    "    def flip_func():\n",
    "        flip_img = tf.image.flip_left_right(img)\n",
    "        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n",
    "                                1 - labels[:, 0],  labels[:, 3],\n",
    "                                labels[:, 4]], axis=1)\n",
    "\n",
    "        return flip_img, flip_labels\n",
    "\n",
    "    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],default=lambda: (img, labels))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _distort 메소드 생성\n",
    "\n",
    "def _distort(img):\n",
    "    img = tf.image.random_brightness(img, 0.4)\n",
    "    img = tf.image.random_contrast(img, 0.5, 1.5)\n",
    "    img = tf.image.random_saturation(img, 0.5, 1.5)\n",
    "    img = tf.image.random_hue(img, 0.1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Prior box 적용 <br><br>\n",
    "\n",
    "SSD 모델을 위한 Prior box 적용 정보는 데이터셋에 반영되어야 합니다. <br><br>\n",
    "\n",
    "\n",
    "prior box 와 bounding box 사이에 IOU 계산을 위해 <br>\n",
    "__자카드 지수__를 측정해야 하므로 다음과 같은 메소드들을 작성하여 활용합니다. <br><br>\n",
    "\n",
    "- _intersect <br>\n",
    "- _jaccard <br><br>\n",
    "\n",
    "\n",
    "이렇게 준비된 jaccard index 를 계산하는 메소드들을 활용해, <br>\n",
    "tfrecord 데이터셋의 라벨을 가공해야 합니다. <br><br>\n",
    "\n",
    "1. _jaccard 메소드를 이용해, <br>\n",
    "    label 의 ground truth bbox 와 가장 overlap 비율이 높은 matched prior 찾기 <br><br>\n",
    "\n",
    "2. _encode_bbox 메소드를 이용해, <br>\n",
    "    bbox 의 sacle 을 동일하게 보정 <br><br>\n",
    "\n",
    "3. 전체 prior box 에 대해서, <br>\n",
    "    일정 threshold 이상 overlap 되는 ground truth bbox 의 존재여부(boolean)를 concat 하여 새로운 label 생성 <br><br>\n",
    "\n",
    "참고. <br>\n",
    "[자카드 거리, 자카드 지수](https://rfriend.tistory.com/318)\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _interset 메소드 생성\n",
    "\n",
    "def _intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2]:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    max_xy = tf.minimum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n",
    "    min_xy = tf.maximum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n",
    "    inter = tf.clip_by_value(max_xy - min_xy, 0.0, 512.0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _jaccard 메소드 생성\n",
    "\n",
    "def _jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = _intersect(box_a, box_b)\n",
    "    area_a = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    area_b = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _encode_bbox 메소드 생성\n",
    "\n",
    "def _encode_bbox(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth\n",
    "    boxes we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n",
    "\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = tf.math.log(g_wh) / variances[1]\n",
    "\n",
    "    # return target for smooth_l1_loss\n",
    "    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_tf 함수 작성\n",
    "\n",
    "def encode_tf(labels, priors, match_thresh, variances=None):\n",
    "    \"\"\"tensorflow encoding\"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    priors = tf.cast(priors, tf.float32)\n",
    "    bbox = labels[:, :4]\n",
    "    conf = labels[:, -1]\n",
    "\n",
    "    # jaccard index\n",
    "    overlaps = _jaccard(bbox, priors)\n",
    "    best_prior_overlap = tf.reduce_max(overlaps, 1)\n",
    "    best_prior_idx = tf.argmax(overlaps, 1, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.reduce_max(overlaps, 0)\n",
    "    best_truth_idx = tf.argmax(overlaps, 0, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.tensor_scatter_nd_update(\n",
    "        best_truth_overlap, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.ones_like(best_prior_idx, tf.float32) * 2.)\n",
    "    best_truth_idx = tf.tensor_scatter_nd_update(\n",
    "        best_truth_idx, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.range(tf.size(best_prior_idx), dtype=tf.int32))\n",
    "\n",
    "    # Scale Ground-Truth Boxes\n",
    "    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\n",
    "    loc_t = _encode_bbox(matches_bbox, priors, variances)\n",
    "    conf_t = tf.gather(conf, best_truth_idx)  # [num_priors]\n",
    "    conf_t = tf.where(tf.less(best_truth_overlap, match_thresh), tf.zeros_like(conf_t), conf_t)\n",
    "\n",
    "    return tf.concat([loc_t, conf_t[..., tf.newaxis]], axis=1)"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Train/Validation set 분할 및 배치 결정 <br><br>\n",
    "\n",
    "```_load_dataset``` <br><br>\n",
    "\n",
    "위에서 작성한 Augmentation 과 Prior box 적용을 위한 메소드를 <br>\n",
    "준비해둔 tfrecord 데이터셋에 적용하여 SSD 학습을 위한 input 데이터셋을 생성하는 최종 메소드 구현 <br><br>\n",
    "\n",
    "- _transform_data <br>\n",
    "    aumemtation과 prior box label을 적용하여 기존 데이터셋 변환 <br><br>\n",
    "\n",
    "- _parse_tfrecord <br>\n",
    "    tfrecord 에 _transform_data 을 적용하는 함수 클로저 생성 <br><br>\n",
    "\n",
    "- load_tfrecord_dataset <br>\n",
    "    tf.data.TFRecordDataset.map() 에 _parse_tfrecord 을 적용하는 실제 데이터셋 변환 메인 메소드 <br><br>\n",
    "    \n",
    "- load_dataset <br>\n",
    "    load_tfrecord_dataset 을 통해 train, validation 데이터셋을 생성하는 최종 메소드\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _transform_data 메소드 생성\n",
    "\n",
    "def _transform_data(img_dim, using_crop,using_flip, using_distort, using_encoding,using_normalizing, priors,\n",
    "                    match_thresh,  variances):\n",
    "    def transform_data(img, labels):\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        if using_crop:\n",
    "        # randomly crop\n",
    "            img, labels = _crop(img, labels)\n",
    "\n",
    "            # padding to square\n",
    "            img = _pad_to_square(img)\n",
    "\n",
    "        # resize and boxes coordinate to percent\n",
    "        img, labels = _resize(img, labels, img_dim)\n",
    "\n",
    "        # randomly left-right flip\n",
    "        if using_flip:\n",
    "            img, labels = _flip(img, labels)\n",
    "\n",
    "        # distort\n",
    "        if using_distort:\n",
    "            img = _distort(img)\n",
    "\n",
    "        # encode labels to feature targets\n",
    "        if using_encoding:\n",
    "            labels = encode_tf(labels=labels, priors=priors, match_thresh=match_thresh, variances=variances)\n",
    "        if using_normalizing:\n",
    "            img=(img/255.0-0.5)/1.0\n",
    "\n",
    "        return img, labels\n",
    "    return transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _parse_tfrecord 메소드 생성\n",
    "\n",
    "def _parse_tfrecord(img_dim,using_crop, using_flip, using_distort,\n",
    "                    using_encoding, using_normalizing,priors, match_thresh,  variances):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string),\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'classes': tf.io.VarLenFeature(tf.int64),\n",
    "            'x_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'x_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'difficult':tf.io.VarLenFeature(tf.int64),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "           }\n",
    "\n",
    "        parsed_example = tf.io.parse_single_example(tfrecord, features)\n",
    "        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n",
    "\n",
    "        width = tf.cast(parsed_example['width'], tf.float32)\n",
    "        height = tf.cast(parsed_example['height'], tf.float32)\n",
    "\n",
    "        labels = tf.sparse.to_dense(parsed_example['classes'])\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "        labels = tf.stack(\n",
    "            [tf.sparse.to_dense(parsed_example['x_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['y_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['x_maxes']),\n",
    "             tf.sparse.to_dense(parsed_example['y_maxes']),labels], axis=1)\n",
    "\n",
    "        img, labels = _transform_data(\n",
    "            img_dim, using_crop,using_flip, using_distort, using_encoding, using_normalizing,priors,\n",
    "            match_thresh,  variances)(img, labels)\n",
    "\n",
    "        return img, labels\n",
    "    return parse_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_tfrecord_dataset\n",
    "\n",
    "def load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\n",
    "                          using_crop=True,using_flip=True, using_distort=True,\n",
    "                          using_encoding=True, using_normalizing=True,\n",
    "                          priors=None, match_thresh=0.45,variances=None,\n",
    "                          shuffle=True, repeat=True,buffer_size=10240):\n",
    "\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    \"\"\"load dataset from tfrecord\"\"\"\n",
    "    if not using_encoding:\n",
    "        assert batch_size == 1\n",
    "    else:\n",
    "        assert priors is not None\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.cache()\n",
    "    if repeat:\n",
    "        raw_dataset = raw_dataset.repeat()\n",
    "    if shuffle:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "\n",
    "    dataset = raw_dataset.map(\n",
    "        _parse_tfrecord(img_dim, using_crop, using_flip, using_distort,\n",
    "                        using_encoding, using_normalizing,priors, match_thresh,  variances),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dataset 메소드 생성\n",
    "\n",
    "def load_dataset(cfg, priors, shuffle=True, buffer_size=10240,train=True):\n",
    "    \"\"\"load dataset\"\"\"\n",
    "    global dataset\n",
    "    if train:\n",
    "        logging.info(\"load train dataset from {}\".format(cfg['dataset_path']))\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['dataset_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=cfg['using_crop'],\n",
    "            using_flip=cfg['using_flip'],\n",
    "            using_distort=cfg['using_distort'],\n",
    "            using_encoding=True,\n",
    "            using_normalizing=cfg['using_normalizing'],\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=True,\n",
    "            buffer_size=buffer_size)\n",
    "    else:\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['val_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=False,\n",
    "            using_flip=False,\n",
    "            using_distort=False,\n",
    "            using_encoding=True,\n",
    "            using_normalizing=True,\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=False,\n",
    "            buffer_size=buffer_size)\n",
    "        logging.info(\"load validation dataset from {}\".format(cfg['val_path']))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "#### 모델 학습을 위한 데이터셋 생성 과정 파이썬 모듈화 <br>\n",
    "\n",
    "모델 학습을 위한 input 데이터셋을 생성하는 과정을 tf_dataloader.py 파일에 모듈화 <br><br>\n",
    "\n",
    "```cd ~/aiffel/face_detector && python tf_dataloader.py```\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## SSD 모델 학습\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Learning rate shceduler <br><br>\n",
    "\n",
    "학습 구간별로 learning rate 가 일정하게 감소하는 함수를 작성하여 학습에 활용 <br><br>\n",
    "\n",
    "PiecewiseConstantDecay 를 상속받아, <br>\n",
    "초기시점에 WarmUp 부분을 추가한 PiecewiseConstantWarmUpDecay 를 활용\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseConstantWarmUpDecay(\n",
    "        tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule wiht warm up schedule.\n",
    "    Modified from tf.keras.optimizers.schedules.PiecewiseConstantDecay\"\"\"\n",
    "\n",
    "    def __init__(self, boundaries, values, warmup_steps, min_lr,\n",
    "                 name=None):\n",
    "        super(PiecewiseConstantWarmUpDecay, self).__init__()\n",
    "\n",
    "        if len(boundaries) != len(values) - 1:\n",
    "            raise ValueError(\n",
    "                    \"The length of boundaries should be 1 less than the\"\n",
    "                    \"length of values\")\n",
    "\n",
    "        self.boundaries = boundaries\n",
    "        self.values = values\n",
    "        self.name = name\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"PiecewiseConstantWarmUp\"):\n",
    "            step = tf.cast(tf.convert_to_tensor(step), tf.float32)\n",
    "            pred_fn_pairs = []\n",
    "            warmup_steps = self.warmup_steps\n",
    "            boundaries = self.boundaries\n",
    "            values = self.values\n",
    "            min_lr = self.min_lr\n",
    "\n",
    "            pred_fn_pairs.append(\n",
    "                (step <= warmup_steps,\n",
    "                 lambda: min_lr + step * (values[0] - min_lr) / warmup_steps))\n",
    "            pred_fn_pairs.append(\n",
    "                (tf.logical_and(step <= boundaries[0],\n",
    "                                step > warmup_steps),\n",
    "                 lambda: tf.constant(values[0])))\n",
    "            pred_fn_pairs.append(\n",
    "                (step > boundaries[-1], lambda: tf.constant(values[-1])))\n",
    "\n",
    "            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n",
    "                                    values[1:-1]):\n",
    "                # Need to bind v here; can do this with lambda v=v: ...\n",
    "                pred = (step > low) & (step <= high)\n",
    "                pred_fn_pairs.append((pred, lambda: tf.constant(v)))\n",
    "\n",
    "            # The default isn't needed here because our conditions are mutually\n",
    "            # exclusive and exhaustive, but tf.case requires it.\n",
    "            return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]),\n",
    "                           exclusive=True)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "                \"boundaries\": self.boundaries,\n",
    "                \"values\": self.values,\n",
    "                \"warmup_steps\": self.warmup_steps,\n",
    "                \"min_lr\": self.min_lr,\n",
    "                \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiStepWarmUpLR(initial_learning_rate, lr_steps, lr_rate,\n",
    "                      warmup_steps=0., min_lr=0.,\n",
    "                      name='MultiStepWarmUpLR'):\n",
    "    \"\"\"Multi-steps warm up learning rate scheduler.\"\"\"\n",
    "    assert warmup_steps <= lr_steps[0]\n",
    "    assert min_lr <= initial_learning_rate\n",
    "    lr_steps_value = [initial_learning_rate]\n",
    "    for _ in range(len(lr_steps)):\n",
    "        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n",
    "    return PiecewiseConstantWarmUpDecay(\n",
    "        boundaries=lr_steps, values=lr_steps_value, warmup_steps=warmup_steps,\n",
    "        min_lr=min_lr)"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Hard negative mining <br><br>\n",
    "\n",
    "Object Detection 모델 학습시 자주 사용되는 Hard negative mining 기법 <br><br>\n",
    "\n",
    "- 학습과정에서 label 은 negative 인데 confidence 가 높게 나오는 샘플을 재학습하면, <br>\n",
    "positive 와 negative 의 모호한 경계선상에 분포한 false negative 오류에 강해진다는 장점이 있습니다. <br><br>\n",
    "\n",
    "- 실제로 confidence 가 높은 샘플을 모아 training을 다시 수행하기보다는, <br>\n",
    "    그런 샘플들에 대한 loss 만 따로 모아 계산해주는 방식으로 반영할 수 있습니다. <br><br>\n",
    "\n",
    "    아래 구현된 hard_negative_mining 메소드와, <br>\n",
    "    이 메소드를 통해 얻은 샘플을 통해 얻은 localization loss 를 <br>\n",
    "    기존의 classification loss 에 추가로 반영하는 MultiBoxLoss 계산 메소드를 확인\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard_negative_mining 함수 작성\n",
    "\n",
    "def hard_negative_mining(loss, class_truth, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        class_truth: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        class_loss: classification loss\n",
    "        loc_loss: regression loss\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # class_truth: B x N\n",
    "    pos_idx = class_truth > 0\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
    "    rank = tf.argsort(rank, axis=1)\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
    "\n",
    "    return pos_idx, neg_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiBoxLoss 함수 작성\n",
    "\n",
    "def MultiBoxLoss(num_class=3, neg_pos_ratio=3.0):\n",
    "    def multi_loss(y_true, y_pred):\n",
    "        \"\"\" Compute losses for SSD\n",
    "               regression loss: smooth L1\n",
    "               classification loss: cross entropy\n",
    "           Args:\n",
    "               y_true: [B,N,5]\n",
    "               y_pred: [B,N,num_class]\n",
    "               class_pred: outputs of classification heads (B,N, num_classes)\n",
    "               loc_pred: outputs of regression heads (B,N, 4)\n",
    "               class_truth: classification targets (B,N)\n",
    "               loc_truth: regression targets (B,N, 4)\n",
    "           Returns:\n",
    "               class_loss: classification loss\n",
    "               loc_loss: regression loss\n",
    "       \"\"\"\n",
    "        num_batch = tf.shape(y_true)[0]\n",
    "        num_prior = tf.shape(y_true)[1]\n",
    "        loc_pred, class_pred = y_pred[..., :4], y_pred[..., 4:]\n",
    "        loc_truth, class_truth = y_true[..., :4], tf.squeeze(y_true[..., 4:])\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses without reduction\n",
    "        temp_loss = cross_entropy(class_truth, class_pred)\n",
    "        # 2. hard negative mining\n",
    "        pos_idx, neg_idx = hard_negative_mining(temp_loss, class_truth, neg_pos_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n",
    "\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        loss_class = cross_entropy(\n",
    "            class_truth[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            class_pred[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # localization loss only consist of positive examples (smooth L1)\n",
    "        loss_loc = smooth_l1_loss(loc_truth[pos_idx],loc_pred[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "\n",
    "        loss_class = loss_class / num_pos\n",
    "        loss_loc = loss_loc / num_pos\n",
    "        return loss_loc, loss_class\n",
    "\n",
    "    return multi_loss"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 모델 학습 진행 <br><br>\n",
    "\n",
    "batch size, epoch 수 등 학습에 대한 기본설정은 cfg dict 내용 확인\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function _parse_tfrecord.<locals>.parse_tfrecord at 0x7fb6100a1440> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: module 'gast' has no attribute 'Index'\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습을 위한 설정\n",
    "\n",
    "global load_t1\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "\n",
    "weights_dir = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "\n",
    "logging.info(\"Load configuration...\")\n",
    "label_classes = cfg['labels_list']\n",
    "logging.info(f\"Total image sample:{cfg['dataset_len']},Total classes number:\"\n",
    "             f\"{len(label_classes)},classes list:{label_classes}\")\n",
    "\n",
    "logging.info(\"Compute prior boxes...\")\n",
    "priors, num_cell = prior_box(cfg)\n",
    "logging.info(f\"Prior boxes number:{len(priors)},default anchor box number per feature map cell:{num_cell}\") # 4420, [3, 2, 2, 3]\n",
    "\n",
    "logging.info(\"Loading dataset...\")\n",
    "train_dataset = load_dataset(cfg, priors, shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    (None, 120, 160, 32) 0           conv_bn_44[0][0]                 \n__________________________________________________________________________________________________\nconv_pad_45 (ZeroPadding2D)     (None, 122, 162, 32) 0           conv_relu_44[0][0]               \n__________________________________________________________________________________________________\nconv_45 (Conv2D)                (None, 60, 80, 32)   9216        conv_pad_45[0][0]                \n__________________________________________________________________________________________________\nconv_bn_45 (BatchNormalization) (None, 60, 80, 32)   128         conv_45[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_45 (ReLU)             (None, 60, 80, 32)   0           conv_bn_45[0][0]                 \n__________________________________________________________________________________________________\nconv_46 (Conv2D)                (None, 60, 80, 32)   9216        conv_relu_45[0][0]               \n__________________________________________________________________________________________________\nconv_bn_46 (BatchNormalization) (None, 60, 80, 32)   128         conv_46[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_46 (ReLU)             (None, 60, 80, 32)   0           conv_bn_46[0][0]                 \n__________________________________________________________________________________________________\nconv_pad_47 (ZeroPadding2D)     (None, 62, 82, 32)   0           conv_relu_46[0][0]               \n__________________________________________________________________________________________________\nconv_47 (Conv2D)                (None, 30, 40, 64)   18432       conv_pad_47[0][0]                \n__________________________________________________________________________________________________\nconv_bn_47 (BatchNormalization) (None, 30, 40, 64)   256         conv_47[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_47 (ReLU)             (None, 30, 40, 64)   0           conv_bn_47[0][0]                 \n__________________________________________________________________________________________________\nconv_48 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_47[0][0]               \n__________________________________________________________________________________________________\nconv_bn_48 (BatchNormalization) (None, 30, 40, 64)   256         conv_48[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_48 (ReLU)             (None, 30, 40, 64)   0           conv_bn_48[0][0]                 \n__________________________________________________________________________________________________\nconv_49 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_48[0][0]               \n__________________________________________________________________________________________________\nconv_bn_49 (BatchNormalization) (None, 30, 40, 64)   256         conv_49[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_49 (ReLU)             (None, 30, 40, 64)   0           conv_bn_49[0][0]                 \n__________________________________________________________________________________________________\nconv_50 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_49[0][0]               \n__________________________________________________________________________________________________\nconv_bn_50 (BatchNormalization) (None, 30, 40, 64)   256         conv_50[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_50 (ReLU)             (None, 30, 40, 64)   0           conv_bn_50[0][0]                 \n__________________________________________________________________________________________________\nconv_pad_51 (ZeroPadding2D)     (None, 32, 42, 64)   0           conv_relu_50[0][0]               \n__________________________________________________________________________________________________\nconv_51 (Conv2D)                (None, 15, 20, 128)  73728       conv_pad_51[0][0]                \n__________________________________________________________________________________________________\nconv_bn_51 (BatchNormalization) (None, 15, 20, 128)  512         conv_51[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_51 (ReLU)             (None, 15, 20, 128)  0           conv_bn_51[0][0]                 \n__________________________________________________________________________________________________\nconv_52 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_51[0][0]               \n__________________________________________________________________________________________________\nconv_bn_52 (BatchNormalization) (None, 15, 20, 128)  512         conv_52[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_52 (ReLU)             (None, 15, 20, 128)  0           conv_bn_52[0][0]                 \n__________________________________________________________________________________________________\nconv_53 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_52[0][0]               \n__________________________________________________________________________________________________\nconv_bn_53 (BatchNormalization) (None, 15, 20, 128)  512         conv_53[0][0]                    \n__________________________________________________________________________________________________\nconv_relu_53 (ReLU)             (None, 15, 20, 128)  0           conv_bn_53[0][0]                 \n__________________________________________________________________________________________________\nconv_pad_54 (ZeroPadding2D)     (None, 17, 22, 128)  0           conv_relu_53[0][0]               \n__________________________________________________________________________________________________\nconv_dw_54 (DepthwiseConv2D)    (None, 8, 10, 128)   1152        conv_pad_54[0][0]                \n__________________________________________________________________________________________________\nconv_dw_54_bn (BatchNormalizati (None, 8, 10, 128)   512         conv_dw_54[0][0]                 \n__________________________________________________________________________________________________\nconv_dw_54_relu (ReLU)          (None, 8, 10, 128)   0           conv_dw_54_bn[0][0]              \n__________________________________________________________________________________________________\nconv_pw_54 (Conv2D)             (None, 8, 10, 256)   32768       conv_dw_54_relu[0][0]            \n__________________________________________________________________________________________________\nconv_pw_54_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_54[0][0]                 \n__________________________________________________________________________________________________\nconv_pw_54_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_54_bn[0][0]              \n__________________________________________________________________________________________________\nconv_dw_55 (DepthwiseConv2D)    (None, 8, 10, 256)   2304        conv_pw_54_relu[0][0]            \n__________________________________________________________________________________________________\nconv_dw_55_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_dw_55[0][0]                 \n__________________________________________________________________________________________________\nconv_dw_55_relu (ReLU)          (None, 8, 10, 256)   0           conv_dw_55_bn[0][0]              \n__________________________________________________________________________________________________\nconv_pw_55 (Conv2D)             (None, 8, 10, 256)   65536       conv_dw_55_relu[0][0]            \n__________________________________________________________________________________________________\nconv_pw_55_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_55[0][0]                 \n__________________________________________________________________________________________________\nconv_pw_55_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_55_bn[0][0]              \n__________________________________________________________________________________________________\nconv_pad_56 (ZeroPadding2D)     (None, 10, 12, 256)  0           conv_pw_55_relu[0][0]            \n__________________________________________________________________________________________________\nconv_dw_56 (DepthwiseConv2D)    (None, 4, 5, 256)    2304        conv_pad_56[0][0]                \n__________________________________________________________________________________________________\nconv_dw_56_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_dw_56[0][0]                 \n__________________________________________________________________________________________________\nconv_dw_56_relu (ReLU)          (None, 4, 5, 256)    0           conv_dw_56_bn[0][0]              \n__________________________________________________________________________________________________\nconv_pw_56 (Conv2D)             (None, 4, 5, 256)    65536       conv_dw_56_relu[0][0]            \n__________________________________________________________________________________________________\nconv_pw_56_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_pw_56[0][0]                 \n__________________________________________________________________________________________________\nconv_pw_56_relu (ReLU)          (None, 4, 5, 256)    0           conv_pw_56_bn[0][0]              \n__________________________________________________________________________________________________\nconv2d_52 (Conv2D)              (None, 30, 40, 16)   9232        conv_relu_50[0][0]               \n__________________________________________________________________________________________________\nconv2d_55 (Conv2D)              (None, 15, 20, 16)   18448       conv_relu_53[0][0]               \n__________________________________________________________________________________________________\nconv2d_58 (Conv2D)              (None, 8, 10, 16)    36880       conv_pw_55_relu[0][0]            \n__________________________________________________________________________________________________\nconv2d_61 (Conv2D)              (None, 4, 5, 16)     36880       conv_pw_56_relu[0][0]            \n__________________________________________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)      (None, 30, 40, 16)   0           conv2d_52[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)      (None, 15, 20, 16)   0           conv2d_55[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_14 (LeakyReLU)      (None, 8, 10, 16)    0           conv2d_58[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_15 (LeakyReLU)      (None, 4, 5, 16)     0           conv2d_61[0][0]                  \n__________________________________________________________________________________________________\nconv2d_53 (Conv2D)              (None, 30, 40, 16)   2320        leaky_re_lu_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_54 (Conv2D)              (None, 30, 40, 32)   18464       conv_relu_50[0][0]               \n__________________________________________________________________________________________________\nconv2d_56 (Conv2D)              (None, 15, 20, 16)   2320        leaky_re_lu_13[0][0]             \n__________________________________________________________________________________________________\nconv2d_57 (Conv2D)              (None, 15, 20, 32)   36896       conv_relu_53[0][0]               \n__________________________________________________________________________________________________\nconv2d_59 (Conv2D)              (None, 8, 10, 16)    2320        leaky_re_lu_14[0][0]             \n__________________________________________________________________________________________________\nconv2d_60 (Conv2D)              (None, 8, 10, 32)    73760       conv_pw_55_relu[0][0]            \n__________________________________________________________________________________________________\nconv2d_62 (Conv2D)              (None, 4, 5, 16)     2320        leaky_re_lu_15[0][0]             \n__________________________________________________________________________________________________\nconv2d_63 (Conv2D)              (None, 4, 5, 32)     73760       conv_pw_56_relu[0][0]            \n__________________________________________________________________________________________________\nconcatenate_12 (Concatenate)    (None, 30, 40, 48)   0           conv2d_53[0][0]                  \n                                                                 conv2d_54[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_13 (Concatenate)    (None, 15, 20, 48)   0           conv2d_56[0][0]                  \n                                                                 conv2d_57[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_14 (Concatenate)    (None, 8, 10, 48)    0           conv2d_59[0][0]                  \n                                                                 conv2d_60[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_15 (Concatenate)    (None, 4, 5, 48)     0           conv2d_62[0][0]                  \n                                                                 conv2d_63[0][0]                  \n__________________________________________________________________________________________________\nre_lu_12 (ReLU)                 (None, 30, 40, 48)   0           concatenate_12[0][0]             \n__________________________________________________________________________________________________\nre_lu_13 (ReLU)                 (None, 15, 20, 48)   0           concatenate_13[0][0]             \n__________________________________________________________________________________________________\nre_lu_14 (ReLU)                 (None, 8, 10, 48)    0           concatenate_14[0][0]             \n__________________________________________________________________________________________________\nre_lu_15 (ReLU)                 (None, 4, 5, 48)     0           concatenate_15[0][0]             \n__________________________________________________________________________________________________\nconv2d_65 (Conv2D)              (None, 30, 40, 12)   5196        re_lu_12[0][0]                   \n__________________________________________________________________________________________________\nconv2d_67 (Conv2D)              (None, 15, 20, 8)    3464        re_lu_13[0][0]                   \n__________________________________________________________________________________________________\nconv2d_69 (Conv2D)              (None, 8, 10, 8)     3464        re_lu_14[0][0]                   \n__________________________________________________________________________________________________\nconv2d_71 (Conv2D)              (None, 4, 5, 12)     5196        re_lu_15[0][0]                   \n__________________________________________________________________________________________________\nconv2d_64 (Conv2D)              (None, 30, 40, 6)    2598        re_lu_12[0][0]                   \n__________________________________________________________________________________________________\nconv2d_66 (Conv2D)              (None, 15, 20, 4)    1732        re_lu_13[0][0]                   \n__________________________________________________________________________________________________\nconv2d_68 (Conv2D)              (None, 8, 10, 4)     1732        re_lu_14[0][0]                   \n__________________________________________________________________________________________________\nconv2d_70 (Conv2D)              (None, 4, 5, 6)      2598        re_lu_15[0][0]                   \n__________________________________________________________________________________________________\nreshape_17 (Reshape)            (None, 3600, 4)      0           conv2d_65[0][0]                  \n__________________________________________________________________________________________________\nreshape_19 (Reshape)            (None, 600, 4)       0           conv2d_67[0][0]                  \n__________________________________________________________________________________________________\nreshape_21 (Reshape)            (None, 160, 4)       0           conv2d_69[0][0]                  \n__________________________________________________________________________________________________\nreshape_23 (Reshape)            (None, 60, 4)        0           conv2d_71[0][0]                  \n__________________________________________________________________________________________________\nreshape_16 (Reshape)            (None, 3600, 2)      0           conv2d_64[0][0]                  \n__________________________________________________________________________________________________\nreshape_18 (Reshape)            (None, 600, 2)       0           conv2d_66[0][0]                  \n__________________________________________________________________________________________________\nreshape_20 (Reshape)            (None, 160, 2)       0           conv2d_68[0][0]                  \n__________________________________________________________________________________________________\nreshape_22 (Reshape)            (None, 60, 2)        0           conv2d_70[0][0]                  \n__________________________________________________________________________________________________\nface_boxes (Concatenate)        (None, 4420, 4)      0           reshape_17[0][0]                 \n                                                                 reshape_19[0][0]                 \n                                                                 reshape_21[0][0]                 \n                                                                 reshape_23[0][0]                 \n__________________________________________________________________________________________________\nface_classes (Concatenate)      (None, 4420, 2)      0           reshape_16[0][0]                 \n                                                                 reshape_18[0][0]                 \n                                                                 reshape_20[0][0]                 \n                                                                 reshape_22[0][0]                 \n__________________________________________________________________________________________________\npredictions (Concatenate)       (None, 4420, 6)      0           face_boxes[0][0]                 \n                                                                 face_classes[0][0]               \n==================================================================================================\nTotal params: 1,038,956\nTrainable params: 1,034,636\nNon-trainable params: 4,320\n__________________________________________________________________________________________________\n('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "\n",
    "logging.info(\"Create Model...\")\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=True)\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file=os.path.join(os.getcwd(), 'model.png'),\n",
    "                              show_shapes=True, show_layer_names=True)\n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "    logging.info(\"Create network failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 데이터셋 준비 및 학습 파라미터 설정\n",
    "\n",
    "if cfg['resume']:\n",
    "    # Training from latest weights\n",
    "    paths = [os.path.join(weights_dir, path)\n",
    "             for path in os.listdir(weights_dir)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    init_epoch = int(os.path.splitext(latest)[0][-3:])\n",
    "\n",
    "else:\n",
    "    init_epoch = -1\n",
    "\n",
    "steps_per_epoch = cfg['dataset_len'] // cfg['batch_size']\n",
    "logging.info(f\"steps_per_epoch:{steps_per_epoch}\")\n",
    "\n",
    "learning_rate = MultiStepWarmUpLR(\n",
    "    initial_learning_rate=cfg['init_lr'],\n",
    "    lr_steps=[e * steps_per_epoch for e in cfg['lr_decay_epoch']],\n",
    "    lr_rate=cfg['lr_rate'],\n",
    "    warmup_steps=cfg['warmup_epoch'] * steps_per_epoch,\n",
    "    min_lr=cfg['min_lr'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=cfg['momentum'], nesterov=True)\n",
    "multi_loss = MultiBoxLoss(num_class=len(label_classes), neg_pos_ratio=3)\n",
    "train_log_dir = 'logs/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 및 optimization 설정\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        losses = {}\n",
    "        losses['reg'] = tf.reduce_sum(model.losses)  #unused. Init for redefine network\n",
    "        losses['loc'], losses['class'] = multi_loss(labels, predictions)\n",
    "        total_loss = tf.add_n([l for l in losses.values()])\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return total_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function train_step at 0x7fb61034c3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch: 1/100 | Batch 402/402 | Batch time 0.159 || Loss: nan | loc loss:nan | class loss:2.103582 \n",
      "Epoch: 1/100  | Epoch time 126.196 || Average Loss: nan\n",
      "Epoch: 2/100 | Batch 402/402 | Batch time 0.127 || Loss: nan | loc loss:nan | class loss:2.081163 \n",
      "Epoch: 2/100  | Epoch time 59.179 || Average Loss: nan\n",
      "Epoch: 3/100 | Batch 402/402 | Batch time 0.169 || Loss: nan | loc loss:nan | class loss:2.042010 \n",
      "Epoch: 3/100  | Epoch time 57.702 || Average Loss: nan\n",
      "Epoch: 4/100 | Batch 402/402 | Batch time 0.113 || Loss: nan | loc loss:nan | class loss:2.095068 \n",
      "Epoch: 4/100  | Epoch time 58.642 || Average Loss: nan\n",
      "Epoch: 5/100 | Batch 402/402 | Batch time 0.153 || Loss: nan | loc loss:nan | class loss:2.123434 \n",
      "Epoch: 5/100  | Epoch time 58.548 || Average Loss: nan\n",
      "Epoch: 6/100 | Batch 402/402 | Batch time 0.162 || Loss: nan | loc loss:nan | class loss:2.095753 \n",
      "Epoch: 6/100  | Epoch time 56.869 || Average Loss: nan\n",
      "Epoch: 7/100 | Batch 402/402 | Batch time 0.115 || Loss: nan | loc loss:nan | class loss:2.017254 \n",
      "Epoch: 7/100  | Epoch time 57.581 || Average Loss: nan\n",
      "Epoch: 8/100 | Batch 402/402 | Batch time 0.136 || Loss: nan | loc loss:nan | class loss:1.945576 \n",
      "Epoch: 8/100  | Epoch time 58.597 || Average Loss: nan\n",
      "Epoch: 9/100 | Batch 402/402 | Batch time 0.161 || Loss: nan | loc loss:nan | class loss:2.009212 \n",
      "Epoch: 9/100  | Epoch time 56.649 || Average Loss: nan\n",
      "Epoch: 10/100 | Batch 402/402 | Batch time 0.128 || Loss: nan | loc loss:nan | class loss:1.952366 \n",
      "Epoch: 10/100  | Epoch time 57.667 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_010.h5<<<<<<<<<<\n",
      "Epoch: 11/100 | Batch 402/402 | Batch time 0.135 || Loss: nan | loc loss:nan | class loss:1.972425 \n",
      "Epoch: 11/100  | Epoch time 57.635 || Average Loss: nan\n",
      "Epoch: 12/100 | Batch 402/402 | Batch time 0.098 || Loss: nan | loc loss:nan | class loss:1.972607 \n",
      "Epoch: 12/100  | Epoch time 58.202 || Average Loss: nan\n",
      "Epoch: 13/100 | Batch 402/402 | Batch time 0.174 || Loss: nan | loc loss:nan | class loss:1.934789 \n",
      "Epoch: 13/100  | Epoch time 57.616 || Average Loss: nan\n",
      "Epoch: 14/100 | Batch 402/402 | Batch time 0.131 || Loss: nan | loc loss:nan | class loss:1.952859 \n",
      "Epoch: 14/100  | Epoch time 57.343 || Average Loss: nan\n",
      "Epoch: 15/100 | Batch 402/402 | Batch time 0.115 || Loss: nan | loc loss:nan | class loss:1.948187 \n",
      "Epoch: 15/100  | Epoch time 57.633 || Average Loss: nan\n",
      "Epoch: 16/100 | Batch 402/402 | Batch time 0.123 || Loss: nan | loc loss:nan | class loss:1.956086 \n",
      "Epoch: 16/100  | Epoch time 57.495 || Average Loss: nan\n",
      "Epoch: 17/100 | Batch 402/402 | Batch time 0.119 || Loss: nan | loc loss:nan | class loss:1.936688 \n",
      "Epoch: 17/100  | Epoch time 56.418 || Average Loss: nan\n",
      "Epoch: 18/100 | Batch 402/402 | Batch time 0.106 || Loss: nan | loc loss:nan | class loss:1.969310 \n",
      "Epoch: 18/100  | Epoch time 56.540 || Average Loss: nan\n",
      "Epoch: 19/100 | Batch 402/402 | Batch time 0.112 || Loss: nan | loc loss:nan | class loss:1.949182 \n",
      "Epoch: 19/100  | Epoch time 56.204 || Average Loss: nan\n",
      "Epoch: 20/100 | Batch 402/402 | Batch time 0.126 || Loss: nan | loc loss:nan | class loss:1.949527 \n",
      "Epoch: 20/100  | Epoch time 57.062 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_020.h5<<<<<<<<<<\n",
      "Epoch: 21/100 | Batch 402/402 | Batch time 0.135 || Loss: nan | loc loss:nan | class loss:1.938365 \n",
      "Epoch: 21/100  | Epoch time 57.178 || Average Loss: nan\n",
      "Epoch: 22/100 | Batch 402/402 | Batch time 0.130 || Loss: nan | loc loss:nan | class loss:1.947260 \n",
      "Epoch: 22/100  | Epoch time 57.931 || Average Loss: nan\n",
      "Epoch: 23/100 | Batch 402/402 | Batch time 0.126 || Loss: nan | loc loss:nan | class loss:1.942122 \n",
      "Epoch: 23/100  | Epoch time 59.945 || Average Loss: nan\n",
      "Epoch: 24/100 | Batch 402/402 | Batch time 0.089 || Loss: nan | loc loss:nan | class loss:1.951138 \n",
      "Epoch: 24/100  | Epoch time 55.078 || Average Loss: nan\n",
      "Epoch: 25/100 | Batch 402/402 | Batch time 0.112 || Loss: nan | loc loss:nan | class loss:1.963588 \n",
      "Epoch: 25/100  | Epoch time 52.326 || Average Loss: nan\n",
      "Epoch: 26/100 | Batch 402/402 | Batch time 0.117 || Loss: nan | loc loss:nan | class loss:1.936738 \n",
      "Epoch: 26/100  | Epoch time 55.181 || Average Loss: nan\n",
      "Epoch: 27/100 | Batch 402/402 | Batch time 0.082 || Loss: nan | loc loss:nan | class loss:1.933834 \n",
      "Epoch: 27/100  | Epoch time 54.892 || Average Loss: nan\n",
      "Epoch: 28/100 | Batch 402/402 | Batch time 0.104 || Loss: nan | loc loss:nan | class loss:1.960316 \n",
      "Epoch: 28/100  | Epoch time 55.246 || Average Loss: nan\n",
      "Epoch: 29/100 | Batch 402/402 | Batch time 0.087 || Loss: nan | loc loss:nan | class loss:1.941495 \n",
      "Epoch: 29/100  | Epoch time 55.874 || Average Loss: nan\n",
      "Epoch: 30/100 | Batch 402/402 | Batch time 0.101 || Loss: nan | loc loss:nan | class loss:1.951790 \n",
      "Epoch: 30/100  | Epoch time 54.926 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_030.h5<<<<<<<<<<\n",
      "Epoch: 31/100 | Batch 402/402 | Batch time 0.105 || Loss: nan | loc loss:nan | class loss:1.925951 \n",
      "Epoch: 31/100  | Epoch time 54.151 || Average Loss: nan\n",
      "Epoch: 32/100 | Batch 402/402 | Batch time 0.159 || Loss: nan | loc loss:nan | class loss:1.918421 \n",
      "Epoch: 32/100  | Epoch time 55.292 || Average Loss: nan\n",
      "Epoch: 33/100 | Batch 402/402 | Batch time 0.112 || Loss: nan | loc loss:nan | class loss:1.938800 \n",
      "Epoch: 33/100  | Epoch time 56.603 || Average Loss: nan\n",
      "Epoch: 34/100 | Batch 402/402 | Batch time 0.110 || Loss: nan | loc loss:nan | class loss:1.933875 \n",
      "Epoch: 34/100  | Epoch time 54.073 || Average Loss: nan\n",
      "Epoch: 35/100 | Batch 402/402 | Batch time 0.118 || Loss: nan | loc loss:nan | class loss:1.945301 \n",
      "Epoch: 35/100  | Epoch time 55.550 || Average Loss: nan\n",
      "Epoch: 36/100 | Batch 402/402 | Batch time 0.086 || Loss: nan | loc loss:nan | class loss:1.945068 \n",
      "Epoch: 36/100  | Epoch time 55.357 || Average Loss: nan\n",
      "Epoch: 37/100 | Batch 402/402 | Batch time 0.134 || Loss: nan | loc loss:nan | class loss:1.946840 \n",
      "Epoch: 37/100  | Epoch time 55.185 || Average Loss: nan\n",
      "Epoch: 38/100 | Batch 402/402 | Batch time 0.133 || Loss: nan | loc loss:nan | class loss:1.947104 \n",
      "Epoch: 38/100  | Epoch time 55.556 || Average Loss: nan\n",
      "Epoch: 39/100 | Batch 402/402 | Batch time 0.109 || Loss: nan | loc loss:nan | class loss:1.931942 \n",
      "Epoch: 39/100  | Epoch time 55.421 || Average Loss: nan\n",
      "Epoch: 40/100 | Batch 402/402 | Batch time 0.112 || Loss: nan | loc loss:nan | class loss:1.923538 \n",
      "Epoch: 40/100  | Epoch time 56.104 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_040.h5<<<<<<<<<<\n",
      "Epoch: 41/100 | Batch 402/402 | Batch time 0.094 || Loss: nan | loc loss:nan | class loss:1.946654 \n",
      "Epoch: 41/100  | Epoch time 54.694 || Average Loss: nan\n",
      "Epoch: 42/100 | Batch 402/402 | Batch time 0.136 || Loss: nan | loc loss:nan | class loss:1.943858 \n",
      "Epoch: 42/100  | Epoch time 54.616 || Average Loss: nan\n",
      "Epoch: 43/100 | Batch 402/402 | Batch time 0.127 || Loss: nan | loc loss:nan | class loss:1.937454 \n",
      "Epoch: 43/100  | Epoch time 54.785 || Average Loss: nan\n",
      "Epoch: 44/100 | Batch 402/402 | Batch time 0.122 || Loss: nan | loc loss:nan | class loss:1.952339 \n",
      "Epoch: 44/100  | Epoch time 56.639 || Average Loss: nan\n",
      "Epoch: 45/100 | Batch 402/402 | Batch time 0.109 || Loss: nan | loc loss:nan | class loss:1.960928 \n",
      "Epoch: 45/100  | Epoch time 56.351 || Average Loss: nan\n",
      "Epoch: 46/100 | Batch 402/402 | Batch time 0.115 || Loss: nan | loc loss:nan | class loss:1.932870 \n",
      "Epoch: 46/100  | Epoch time 56.513 || Average Loss: nan\n",
      "Epoch: 47/100 | Batch 402/402 | Batch time 0.120 || Loss: nan | loc loss:nan | class loss:1.949215 \n",
      "Epoch: 47/100  | Epoch time 57.304 || Average Loss: nan\n",
      "Epoch: 48/100 | Batch 402/402 | Batch time 0.133 || Loss: nan | loc loss:nan | class loss:1.956504 \n",
      "Epoch: 48/100  | Epoch time 55.409 || Average Loss: nan\n",
      "Epoch: 49/100 | Batch 402/402 | Batch time 0.106 || Loss: nan | loc loss:nan | class loss:1.955072 \n",
      "Epoch: 49/100  | Epoch time 54.727 || Average Loss: nan\n",
      "Epoch: 50/100 | Batch 402/402 | Batch time 0.142 || Loss: nan | loc loss:nan | class loss:1.933078 \n",
      "Epoch: 50/100  | Epoch time 54.253 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_050.h5<<<<<<<<<<\n",
      "Epoch: 51/100 | Batch 402/402 | Batch time 0.121 || Loss: nan | loc loss:nan | class loss:1.913484 \n",
      "Epoch: 51/100  | Epoch time 54.792 || Average Loss: nan\n",
      "Epoch: 52/100 | Batch 402/402 | Batch time 0.141 || Loss: nan | loc loss:nan | class loss:1.912532 \n",
      "Epoch: 52/100  | Epoch time 54.961 || Average Loss: nan\n",
      "Epoch: 53/100 | Batch 402/402 | Batch time 0.116 || Loss: nan | loc loss:nan | class loss:1.911069 \n",
      "Epoch: 53/100  | Epoch time 55.007 || Average Loss: nan\n",
      "Epoch: 54/100 | Batch 402/402 | Batch time 0.111 || Loss: nan | loc loss:nan | class loss:1.911875 \n",
      "Epoch: 54/100  | Epoch time 55.137 || Average Loss: nan\n",
      "Epoch: 55/100 | Batch 402/402 | Batch time 0.163 || Loss: nan | loc loss:nan | class loss:1.911465 \n",
      "Epoch: 55/100  | Epoch time 56.217 || Average Loss: nan\n",
      "Epoch: 56/100 | Batch 402/402 | Batch time 0.131 || Loss: nan | loc loss:nan | class loss:1.910982 \n",
      "Epoch: 56/100  | Epoch time 55.690 || Average Loss: nan\n",
      "Epoch: 57/100 | Batch 402/402 | Batch time 0.095 || Loss: nan | loc loss:nan | class loss:1.910746 \n",
      "Epoch: 57/100  | Epoch time 54.975 || Average Loss: nan\n",
      "Epoch: 58/100 | Batch 402/402 | Batch time 0.113 || Loss: nan | loc loss:nan | class loss:1.911051 \n",
      "Epoch: 58/100  | Epoch time 54.498 || Average Loss: nan\n",
      "Epoch: 59/100 | Batch 402/402 | Batch time 0.131 || Loss: nan | loc loss:nan | class loss:1.911105 \n",
      "Epoch: 59/100  | Epoch time 55.081 || Average Loss: nan\n",
      "Epoch: 60/100 | Batch 402/402 | Batch time 0.120 || Loss: nan | loc loss:nan | class loss:1.913764 \n",
      "Epoch: 60/100  | Epoch time 55.096 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_060.h5<<<<<<<<<<\n",
      "Epoch: 61/100 | Batch 402/402 | Batch time 0.152 || Loss: nan | loc loss:nan | class loss:1.913425 \n",
      "Epoch: 61/100  | Epoch time 54.873 || Average Loss: nan\n",
      "Epoch: 62/100 | Batch 402/402 | Batch time 0.121 || Loss: nan | loc loss:nan | class loss:1.913083 \n",
      "Epoch: 62/100  | Epoch time 54.903 || Average Loss: nan\n",
      "Epoch: 63/100 | Batch 402/402 | Batch time 0.129 || Loss: nan | loc loss:nan | class loss:1.911203 \n",
      "Epoch: 63/100  | Epoch time 54.945 || Average Loss: nan\n",
      "Epoch: 64/100 | Batch 402/402 | Batch time 0.175 || Loss: nan | loc loss:nan | class loss:1.910605 \n",
      "Epoch: 64/100  | Epoch time 55.618 || Average Loss: nan\n",
      "Epoch: 65/100 | Batch 402/402 | Batch time 0.133 || Loss: nan | loc loss:nan | class loss:1.911216 \n",
      "Epoch: 65/100  | Epoch time 55.882 || Average Loss: nan\n",
      "Epoch: 66/100 | Batch 402/402 | Batch time 0.125 || Loss: nan | loc loss:nan | class loss:1.911366 \n",
      "Epoch: 66/100  | Epoch time 55.823 || Average Loss: nan\n",
      "Epoch: 67/100 | Batch 402/402 | Batch time 0.112 || Loss: nan | loc loss:nan | class loss:1.913265 \n",
      "Epoch: 67/100  | Epoch time 55.341 || Average Loss: nan\n",
      "Epoch: 68/100 | Batch 402/402 | Batch time 0.114 || Loss: nan | loc loss:nan | class loss:1.911557 \n",
      "Epoch: 68/100  | Epoch time 56.424 || Average Loss: nan\n",
      "Epoch: 69/100 | Batch 402/402 | Batch time 0.130 || Loss: nan | loc loss:nan | class loss:1.912116 \n",
      "Epoch: 69/100  | Epoch time 56.365 || Average Loss: nan\n",
      "Epoch: 70/100 | Batch 402/402 | Batch time 0.092 || Loss: nan | loc loss:nan | class loss:1.911485 \n",
      "Epoch: 70/100  | Epoch time 55.220 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_070.h5<<<<<<<<<<\n",
      "Epoch: 71/100 | Batch 402/402 | Batch time 0.141 || Loss: nan | loc loss:nan | class loss:1.909840 \n",
      "Epoch: 71/100  | Epoch time 53.419 || Average Loss: nan\n",
      "Epoch: 72/100 | Batch 402/402 | Batch time 0.104 || Loss: nan | loc loss:nan | class loss:1.909717 \n",
      "Epoch: 72/100  | Epoch time 53.114 || Average Loss: nan\n",
      "Epoch: 73/100 | Batch 402/402 | Batch time 0.143 || Loss: nan | loc loss:nan | class loss:1.909534 \n",
      "Epoch: 73/100  | Epoch time 52.646 || Average Loss: nan\n",
      "Epoch: 74/100 | Batch 402/402 | Batch time 0.133 || Loss: nan | loc loss:nan | class loss:1.909726 \n",
      "Epoch: 74/100  | Epoch time 52.427 || Average Loss: nan\n",
      "Epoch: 75/100 | Batch 402/402 | Batch time 0.130 || Loss: nan | loc loss:nan | class loss:1.909724 \n",
      "Epoch: 75/100  | Epoch time 52.039 || Average Loss: nan\n",
      "Epoch: 76/100 | Batch 402/402 | Batch time 0.101 || Loss: nan | loc loss:nan | class loss:1.909742 \n",
      "Epoch: 76/100  | Epoch time 51.949 || Average Loss: nan\n",
      "Epoch: 77/100 | Batch 402/402 | Batch time 0.096 || Loss: nan | loc loss:nan | class loss:1.909723 \n",
      "Epoch: 77/100  | Epoch time 52.793 || Average Loss: nan\n",
      "Epoch: 78/100 | Batch 402/402 | Batch time 0.110 || Loss: nan | loc loss:nan | class loss:1.909741 \n",
      "Epoch: 78/100  | Epoch time 53.371 || Average Loss: nan\n",
      "Epoch: 79/100 | Batch 402/402 | Batch time 0.117 || Loss: nan | loc loss:nan | class loss:1.909811 \n",
      "Epoch: 79/100  | Epoch time 53.411 || Average Loss: nan\n",
      "Epoch: 80/100 | Batch 402/402 | Batch time 0.127 || Loss: nan | loc loss:nan | class loss:1.910172 \n",
      "Epoch: 80/100  | Epoch time 52.950 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_080.h5<<<<<<<<<<\n",
      "Epoch: 81/100 | Batch 402/402 | Batch time 0.123 || Loss: nan | loc loss:nan | class loss:1.909699 \n",
      "Epoch: 81/100  | Epoch time 53.073 || Average Loss: nan\n",
      "Epoch: 82/100 | Batch 402/402 | Batch time 0.131 || Loss: nan | loc loss:nan | class loss:1.909784 \n",
      "Epoch: 82/100  | Epoch time 53.831 || Average Loss: nan\n",
      "Epoch: 83/100 | Batch 402/402 | Batch time 0.128 || Loss: nan | loc loss:nan | class loss:1.909741 \n",
      "Epoch: 83/100  | Epoch time 55.095 || Average Loss: nan\n",
      "Epoch: 84/100 | Batch 402/402 | Batch time 0.121 || Loss: nan | loc loss:nan | class loss:1.909733 \n",
      "Epoch: 84/100  | Epoch time 55.586 || Average Loss: nan\n",
      "Epoch: 85/100 | Batch 402/402 | Batch time 0.122 || Loss: nan | loc loss:nan | class loss:1.909717 \n",
      "Epoch: 85/100  | Epoch time 55.019 || Average Loss: nan\n",
      "Epoch: 86/100 | Batch 402/402 | Batch time 0.098 || Loss: nan | loc loss:nan | class loss:1.909697 \n",
      "Epoch: 86/100  | Epoch time 55.301 || Average Loss: nan\n",
      "Epoch: 87/100 | Batch 402/402 | Batch time 0.181 || Loss: nan | loc loss:nan | class loss:1.909852 \n",
      "Epoch: 87/100  | Epoch time 55.811 || Average Loss: nan\n",
      "Epoch: 88/100 | Batch 402/402 | Batch time 0.160 || Loss: nan | loc loss:nan | class loss:1.909728 \n",
      "Epoch: 88/100  | Epoch time 55.446 || Average Loss: nan\n",
      "Epoch: 89/100 | Batch 402/402 | Batch time 0.103 || Loss: nan | loc loss:nan | class loss:1.909780 \n",
      "Epoch: 89/100  | Epoch time 55.314 || Average Loss: nan\n",
      "Epoch: 90/100 | Batch 402/402 | Batch time 0.143 || Loss: nan | loc loss:nan | class loss:1.909692 \n",
      "Epoch: 90/100  | Epoch time 54.746 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_090.h5<<<<<<<<<<\n",
      "Epoch: 91/100 | Batch 402/402 | Batch time 0.099 || Loss: nan | loc loss:nan | class loss:1.909905 \n",
      "Epoch: 91/100  | Epoch time 55.304 || Average Loss: nan\n",
      "Epoch: 92/100 | Batch 402/402 | Batch time 0.117 || Loss: nan | loc loss:nan | class loss:1.909720 \n",
      "Epoch: 92/100  | Epoch time 55.156 || Average Loss: nan\n",
      "Epoch: 93/100 | Batch 402/402 | Batch time 0.120 || Loss: nan | loc loss:nan | class loss:1.909790 \n",
      "Epoch: 93/100  | Epoch time 54.535 || Average Loss: nan\n",
      "Epoch: 94/100 | Batch 402/402 | Batch time 0.141 || Loss: nan | loc loss:nan | class loss:1.909818 \n",
      "Epoch: 94/100  | Epoch time 55.706 || Average Loss: nan\n",
      "Epoch: 95/100 | Batch 402/402 | Batch time 0.112 || Loss: nan | loc loss:nan | class loss:1.909809 \n",
      "Epoch: 95/100  | Epoch time 55.267 || Average Loss: nan\n",
      "Epoch: 96/100 | Batch 402/402 | Batch time 0.119 || Loss: nan | loc loss:nan | class loss:1.909621 \n",
      "Epoch: 96/100  | Epoch time 55.144 || Average Loss: nan\n",
      "Epoch: 97/100 | Batch 402/402 | Batch time 0.121 || Loss: nan | loc loss:nan | class loss:1.909755 \n",
      "Epoch: 97/100  | Epoch time 54.613 || Average Loss: nan\n",
      "Epoch: 98/100 | Batch 402/402 | Batch time 0.141 || Loss: nan | loc loss:nan | class loss:1.909852 \n",
      "Epoch: 98/100  | Epoch time 54.776 || Average Loss: nan\n",
      "Epoch: 99/100 | Batch 402/402 | Batch time 0.122 || Loss: nan | loc loss:nan | class loss:1.909793 \n",
      "Epoch: 99/100  | Epoch time 56.082 || Average Loss: nan\n",
      "Epoch: 100/100 | Batch 402/402 | Batch time 0.136 || Loss: nan | loc loss:nan | class loss:1.909653 \n",
      "Epoch: 100/100  | Epoch time 55.027 || Average Loss: nan\n",
      ">>>>>>>>>>Save weights file at /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_100.h5<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "# 1 epoch 당 1 분 가량 소요 (총 100 epoch)\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(init_epoch+1,cfg['epoch']):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        avg_loss = 0.0\n",
    "        for step, (inputs, labels) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "\n",
    "            load_t0 = time.time()\n",
    "            total_loss, losses = train_step(inputs, labels)\n",
    "            avg_loss = (avg_loss * step + total_loss.numpy()) / (step + 1)\n",
    "            load_t1 = time.time()\n",
    "            batch_time = load_t1 - load_t0\n",
    "\n",
    "            steps =steps_per_epoch*epoch+step\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss/total_loss', total_loss, step=steps)\n",
    "                for k, l in losses.items():\n",
    "                    tf.summary.scalar('loss/{}'.format(k), l, step=steps)\n",
    "                tf.summary.scalar('learning_rate', optimizer.lr(steps), step=steps)\n",
    "\n",
    "            print(f\"\\rEpoch: {epoch + 1}/{cfg['epoch']} | Batch {step + 1}/{steps_per_epoch} | Batch time {batch_time:.3f} || Loss: {total_loss:.6f} | loc loss:{losses['loc']:.6f} | class loss:{losses['class']:.6f} \",end = '',flush=True)\n",
    "\n",
    "        print(f\"\\nEpoch: {epoch + 1}/{cfg['epoch']}  | Epoch time {(load_t1 - start):.3f} || Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/avg_loss',avg_loss,step=epoch)\n",
    "\n",
    "        if (epoch + 1) % cfg['save_freq'] == 0:\n",
    "            filepath = os.path.join(weights_dir, f'weights_epoch_{(epoch + 1):03d}.h5')\n",
    "            model.save_weights(filepath)\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\">>>>>>>>>>Save weights file at {filepath}<<<<<<<<<<\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        exit(0)"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "#### 모델 학습 과정 파이썬 모듈화 <br>\n",
    "\n",
    "모델을 학습하기 위한 세팅 과정과 학습하는 과정을 train.py 파일에 모듈화 <br><br>\n",
    "\n",
    "```cd ~/aiffel/face_detector && python train.py```\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 추론 (Inference)\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Non-Max Suppression (NMS) <br><br>\n",
    "\n",
    "\n",
    "Grid cell 을 사용하는 Object Detection 의 추론(inference) 단계에서는 <br><br>\n",
    "\n",
    "하나의 obejct 가 여러개의 prior box 에 걸쳐 있을 경우, <br>\n",
    "확률이 가장 높은 1 개의 prior box 하나로 줄여주는 NMS (Non-Max Suppression) 이 필요\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_bbox_tf 함수 작성\n",
    "\n",
    "def decode_bbox_tf(pre, priors, variances=None):\n",
    "    \"\"\"Decode locations from predictions using prior to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): location predictions for loc layers,\n",
    "            Shape: [num_prior,4]\n",
    "        prior (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_prior,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        decoded bounding box predictions xmin, ymin, xmax, ymax\n",
    "    \"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "    centers = priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:]\n",
    "    sides = priors[:, 2:] * tf.math.exp(pre[:, 2:] * variances[1])\n",
    "\n",
    "    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_nms 함수 작성\n",
    "\n",
    "def compute_nms(boxes, scores, nms_threshold=0.5, limit=200):\n",
    "    \"\"\" Perform Non Maximum Suppression algorithm\n",
    "        to eliminate boxes with high overlap\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        scores: tensor (num_boxes,)\n",
    "        nms_threshold: NMS threshold\n",
    "        limit: maximum number of boxes to keep\n",
    "    Returns:\n",
    "        idx: indices of kept boxes\n",
    "    \"\"\"\n",
    "    if boxes.shape[0] == 0:\n",
    "        return tf.constant([], dtype=tf.int32)\n",
    "    selected = [0]\n",
    "    idx = tf.argsort(scores, direction='DESCENDING')\n",
    "    idx = idx[:limit]\n",
    "    boxes = tf.gather(boxes, idx)\n",
    "\n",
    "    iou = _jaccard(boxes, boxes)\n",
    "\n",
    "    while True:\n",
    "        row = iou[selected[-1]]\n",
    "        next_indices = row <= nms_threshold\n",
    "\n",
    "        iou = tf.where(\n",
    "            tf.expand_dims(tf.math.logical_not(next_indices), 0),\n",
    "            tf.ones_like(iou, dtype=tf.float32),\n",
    "            iou)\n",
    "\n",
    "        if not tf.math.reduce_any(next_indices):\n",
    "            break\n",
    "\n",
    "        selected.append(tf.argsort(\n",
    "            tf.dtypes.cast(next_indices, tf.int32), direction='DESCENDING')[0].numpy())\n",
    "\n",
    "    return tf.gather(idx, selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_predict 함수 작성\n",
    "\n",
    "def parse_predict(predictions, priors, cfg):\n",
    "    label_classes = cfg['labels_list']\n",
    "\n",
    "    bbox_regressions, confs = tf.split(predictions[0], [4, -1], axis=-1)\n",
    "    boxes = decode_bbox_tf(bbox_regressions, priors, cfg['variances'])\n",
    "\n",
    "\n",
    "    confs = tf.math.softmax(confs, axis=-1)\n",
    "\n",
    "    out_boxes = []\n",
    "    out_labels = []\n",
    "    out_scores = []\n",
    "\n",
    "    for c in range(1, len(label_classes)):\n",
    "        cls_scores = confs[:, c]\n",
    "\n",
    "        score_idx = cls_scores > cfg['score_threshold']\n",
    "\n",
    "        cls_boxes = boxes[score_idx]\n",
    "        cls_scores = cls_scores[score_idx]\n",
    "\n",
    "        nms_idx = compute_nms(cls_boxes, cls_scores, cfg['nms_threshold'], cfg['max_number_keep'])\n",
    "\n",
    "        cls_boxes = tf.gather(cls_boxes, nms_idx)\n",
    "        cls_scores = tf.gather(cls_scores, nms_idx)\n",
    "\n",
    "        cls_labels = [c] * cls_boxes.shape[0]\n",
    "\n",
    "        out_boxes.append(cls_boxes)\n",
    "        out_labels.extend(cls_labels)\n",
    "        out_scores.append(cls_scores)\n",
    "\n",
    "    out_boxes = tf.concat(out_boxes, axis=0)\n",
    "    out_scores = tf.concat(out_scores, axis=0)\n",
    "\n",
    "    boxes = tf.clip_by_value(out_boxes, 0.0, 1.0).numpy()\n",
    "    classes = np.array(out_labels)\n",
    "    scores = out_scores.numpy()\n",
    "\n",
    "    return boxes, classes, scores"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 모델을 통한 추론 <br><br>\n",
    "\n",
    "학습된 SSD 모델을 통하여 Multi-face Detection 태스크를 수행해 봅시다. <br><br>\n",
    "\n",
    "- 추론하는데 사용되는 모델의 입력으로 이미지 데이터를 전달하기 위해 필요한 함수와 <br>\n",
    "    추론 결과를 시각화하며 확인하기 위해 원래의 이미지 데이터로 되돌리기 위한 함수를 작성 <br>\n",
    "- 추론 결과를 시각화하여 확인하는 함수 작성\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론을 위해 필요한 pad_input_image 함수 작성\n",
    "\n",
    "def pad_input_image(img, max_steps):\n",
    "    \"\"\"\n",
    "        pad image to suitable shape\n",
    "    \"\"\"\n",
    "    img_h, img_w, _ = img.shape\n",
    "\n",
    "    img_pad_h = 0\n",
    "    if img_h % max_steps > 0:\n",
    "        img_pad_h = max_steps - img_h % max_steps\n",
    "\n",
    "    img_pad_w = 0\n",
    "    if img_w % max_steps > 0:\n",
    "        img_pad_w = max_steps - img_w % max_steps\n",
    "\n",
    "    padd_val = np.mean(img, axis=(0, 1)).astype(np.uint8)\n",
    "    img = cv2.copyMakeBorder(img, 0, img_pad_h, 0, img_pad_w,\n",
    "                             cv2.BORDER_CONSTANT, value=padd_val.tolist())\n",
    "    pad_params = (img_h, img_w, img_pad_h, img_pad_w)\n",
    "\n",
    "    return img, pad_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론된 결과를 되돌리는 recover_pad_output 함수 작성\n",
    "\n",
    "def recover_pad_output(outputs, pad_params):\n",
    "    \"\"\"\n",
    "        recover the padded output effect\n",
    "    \"\"\"\n",
    "    img_h, img_w, img_pad_h, img_pad_w = pad_params\n",
    "\n",
    "    recover_xy = np.reshape(outputs[0], [-1, 2, 2]) * \\\n",
    "                 [(img_pad_w + img_w) / img_w, (img_pad_h + img_h) / img_h]\n",
    "    outputs[0] = np.reshape(recover_xy, [-1, 4])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 화면 출력을 위한 show_image 함수 작성\n",
    "\n",
    "def show_image(img, boxes, classes, scores, img_height, img_width, prior_index, class_list):\n",
    "    \"\"\"\n",
    "    draw bboxes and labels\n",
    "    out:boxes,classes,scores\n",
    "    \"\"\"\n",
    "    # bbox\n",
    "\n",
    "    x1, y1, x2, y2 = int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height), \\\n",
    "                     int(boxes[prior_index][2] * img_width), int(boxes[prior_index][3] * img_height)\n",
    "    if classes[prior_index] == 1:\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        color = (0, 0, 255)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "    # confidence\n",
    "\n",
    "    # if scores:\n",
    "    #   score = \"{:.4f}\".format(scores[prior_index])\n",
    "    #   class_name = class_list[classes[prior_index]]\n",
    "\n",
    "    #  cv2.putText(img, '{} {}'.format(class_name, score),\n",
    "    #              (int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height) - 4),\n",
    "    #              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 추론 결과 확인 <br><br>\n",
    "\n",
    "\n",
    "여러 사람의 얼굴이 포함된 테스트용 이미지를 <br>\n",
    "~/aiffel/face_detector/image.png 경로에 저장 후 아래 코드 실행\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model path : /home/ssac29/aiffel/face_detector/checkpoints/weights_epoch_100.h5\n[*] Predict /home/ssac29/aiffel/face_detector/image.png image.. \n(256, 320, 3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nprint(f\"scores:{scores}\")\\n# recover padding effect\\nboxes = recover_pad_output(boxes, pad_params)\\n\\n# draw and save results\\nsave_img_path = os.path.join(\\'assets/out_\\' + os.path.basename(img_path))\\n\\nfor prior_index in range(len(boxes)):\\n    show_image(img_raw, boxes, classes, scores, img_height_raw, img_width_raw, prior_index, cfg[\\'labels_list\\'])\\n\\ncv2.imwrite(save_img_path, img_raw)\\ncv2.imshow(\\'results\\', img_raw)\\nif cv2.waitKey(0) == ord(\\'q\\'):\\n    exit(0)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "# 이미지에서 multi-face detection 추론 후 시각화\n",
    "\n",
    "import cv2\n",
    "\n",
    "global model\n",
    "min_sizes = cfg['min_sizes']\n",
    "num_cell = [len(min_sizes[k]) for k in range(len(cfg['steps']))]\n",
    "model_path = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "img_path = os.getenv('HOME')+'/aiffel/face_detector/image.png'\n",
    "\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=False)\n",
    "\n",
    "    paths = [os.path.join(model_path, path)\n",
    "             for path in os.listdir(model_path)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    print(f\"model path : {latest}\")\n",
    "\n",
    "except AttributeError as e:\n",
    "    print('Please make sure there is at least one weights at {}'.format(model_path))\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    print(f\"Cannot find image path from {img_path}\")\n",
    "    exit()\n",
    "print(\"[*] Predict {} image.. \".format(img_path))\n",
    "img_raw = cv2.imread(img_path)\n",
    "img_raw = cv2.resize(img_raw, (320, 240))\n",
    "img_height_raw, img_width_raw, _ = img_raw.shape\n",
    "img = np.float32(img_raw.copy())\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# pad input image to avoid unmatched shape problem\n",
    "img, pad_params = pad_input_image(img, max_steps=max(cfg['steps']))\n",
    "img = img / 255.0 - 0.5\n",
    "print(img.shape)\n",
    "priors, _ = prior_box(cfg, image_sizes=(img.shape[0], img.shape[1]))\n",
    "priors = tf.cast(priors, tf.float32)\n",
    "\n",
    "predictions = model.predict(img[np.newaxis, ...])\n",
    "\n",
    "boxes, classes, scores = parse_predict(predictions, priors, cfg)\n",
    "\n",
    "print(f\"scores:{scores}\")\n",
    "# recover padding effect\n",
    "boxes = recover_pad_output(boxes, pad_params)\n",
    "\n",
    "# draw and save results\n",
    "save_img_path = os.path.join('assets/out_' + os.path.basename(img_path))\n",
    "\n",
    "for prior_index in range(len(boxes)):\n",
    "    show_image(img_raw, boxes, classes, scores, img_height_raw, img_width_raw, prior_index, cfg['labels_list'])\n",
    "\n",
    "cv2.imwrite(save_img_path, img_raw)\n",
    "cv2.imshow('results', img_raw)\n",
    "if cv2.waitKey(0) == ord('q'):\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(boxes)"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "#### 모델을 이용한 이미지 추론 파이썬 모듈화 <br>\n",
    "\n",
    "학습된 모델을 이용하여 이미지에서 얼굴을 디텍션하는 과정을 inference.py 파일에 모듈화 <br><br>\n",
    "\n",
    "```cd ~/aiffel/face_detector && python inference.py  checkpoints/  image.png ```\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}